{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cora_Node_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhKn6aXSwPTFU26P0oy3VN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AslantheAslan/Node-classification-by-several-methods/blob/main/Cora_Node_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULxRw8ME7Th",
        "outputId": "d5244da3-833a-4eb0-ee69-3068dac18b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.genfromtxt(r'/content/drive/My Drive/Colab Notebooks/Data/cora_data/edges.csv', delimiter=',', dtype=int)\n",
        "print(type(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM9dI1QNGBK6",
        "outputId": "6725d132-89b2-47a5-8d37-d046154b8043"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpsuoT7dGhQC",
        "outputId": "044d0464-5a2c-4a33-a202-67e62c41aeb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5429, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges = list(map(tuple, a))"
      ],
      "metadata": {
        "id": "IiOef5mxLyoR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = np.dtype([('nodes', np.integer), ('classes', np.unicode_, 25)])"
      ],
      "metadata": {
        "id": "Gdx5mplUOQ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Data/cora_data/group-edges.csv\")\n",
        "df[\"Class\"] = df[\"Class\"].map({\"Rule_Learning\": 0, \"Neural_Networks\": 1, \"Theory\": 2, \"Case_Based\": 3, \"Probabilistic_Methods\": 4, \"Genetic_Algorithms\": 5, \"Reinforcement_Learning\": 6})\n",
        "df['Class'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIeWiAUXfoeg",
        "outputId": "b934c0f6-67d6-4417-98df-71ed1bb842ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    818\n",
              "4    426\n",
              "5    418\n",
              "2    351\n",
              "3    298\n",
              "6    217\n",
              "0    180\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = df.to_numpy(dtype=int)\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eagZ8MQxEZn",
        "outputId": "7aa02ad4-5d2c-4c53-fe0c-86c4f96c5859"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1000012,       0],\n",
              "       [ 100197,       1],\n",
              "       [ 100701,       3],\n",
              "       ...,\n",
              "       [  99023,       1],\n",
              "       [  99025,       1],\n",
              "       [  99030,       1]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(map(tuple, b))\n",
        "len(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLJBosetzJfG",
        "outputId": "491ec2b4-07d3-4b41-ac08-569a751a8935"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2708"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enumerating the data samples once again since we need to assign nodes numbers from 0 to 2707 adjacently.\n",
        "\n",
        "for i in range(len(b)):\n",
        "  k = b[i][0]\n",
        "  for j in range(len(a)):\n",
        "    for m in range(2):\n",
        "      if a[j][m] == k:\n",
        "        a[j][m] = i + 2000000\n",
        "  b[i][0] = i"
      ],
      "metadata": {
        "id": "XpL_5cffOtdl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = a\n",
        "for i in range(len(a)):\n",
        "  for j in range(2):\n",
        "    a1[i][j] = a[i][j] - 2000000"
      ],
      "metadata": {
        "id": "GPT40RNHr3J9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges = list(map(tuple, a1))\n",
        "classes = list(map(tuple, b))"
      ],
      "metadata": {
        "id": "BXZvyQgFtDTd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges"
      ],
      "metadata": {
        "id": "M5BPA_z2tLsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "id": "2f2YenoIPu4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku"
      ],
      "metadata": {
        "id": "4Ta8Y93nzVq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "import flax\n",
        "import haiku as hk\n",
        "import optax\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "Rf5GrjUazfaW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cora_dataset() -> jraph.GraphsTuple:\n",
        "  \"\"\"Returns GraphsTuple representing cora dataset.\"\"\"\n",
        "  social_graph = edges\n",
        "  # Add reverse edges.\n",
        "  social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
        "  n_club_members = 2708\n",
        "\n",
        "  return jraph.GraphsTuple(\n",
        "      n_node=jnp.asarray([n_club_members]),\n",
        "      n_edge=jnp.asarray([len(social_graph)]),\n",
        "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
        "      nodes=jnp.eye(n_club_members),\n",
        "      # No edge features.\n",
        "      edges=None,\n",
        "      globals=None,\n",
        "      senders=jnp.asarray([edge[0] for edge in social_graph]),\n",
        "      receivers=jnp.asarray([edge[1] for edge in social_graph]))\n",
        "\n",
        "def get_ground_truth_assignments_for_cora_dataset() -> jnp.ndarray:\n",
        "  \"\"\"Returns ground truth assignments for cora dataset.\"\"\"\n",
        "  return jnp.asarray([element[1] for element in classes])"
      ],
      "metadata": {
        "id": "-FIWOAY4zivG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = get_cora_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdacMDXuzmke",
        "outputId": "8287feba-457d-41fe-ce24-d9d05934820a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes: {graph.n_node[0]}')\n",
        "print(f'Number of edges: {graph.n_edge[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahZ1A5tAzo9e",
        "outputId": "d88da888-618c-493a-b11f-e6ae3f8aaccd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 2708\n",
            "Number of edges: 10858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_jraph_to_networkx_graph(jraph_graph):\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph\n",
        "\n",
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple):\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(nx_graph, pos=pos, with_labels = True, node_size=500, font_color='yellow')"
      ],
      "metadata": {
        "id": "mgGiLxfz0YWf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Only make it work when you need to visualize the whole graph. \n",
        "# Note that it costs too much of computation time on Google Colab.\n",
        "\n",
        "nx_graph = convert_jraph_to_networkx_graph(graph)\n",
        "pos = nx.random_layout(nx_graph)\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "nx.draw(nx_graph, pos=pos, with_labels = True, node_size=100, font_color='yellow')\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RTR3yX1O0bqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here, the necessary functions to implement a GCN were defined\n",
        "\"\"\"\n",
        "\n",
        "class MLP(hk.Module):\n",
        "  def __init__(self, features: jnp.ndarray):\n",
        "    super().__init__()\n",
        "    self.features = features\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    layers = []\n",
        "    for feat in self.features[:-1]:\n",
        "      layers.append(hk.Linear(feat))\n",
        "      layers.append(jax.nn.relu)\n",
        "    layers.append(hk.Linear(self.features[-1]))\n",
        "\n",
        "    mlp = hk.Sequential(layers)\n",
        "    return mlp(x)\n",
        "\n",
        "def apply_simplified_gcn(graph: jraph.GraphsTuple):\n",
        "  # Unpack GraphsTuple\n",
        "  nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "  # 1. Update node features\n",
        "  # For simplicity, we will first use an identify function here, and replace it\n",
        "  # with a trainable MLP block later.\n",
        "  update_node_fn = lambda nodes: nodes\n",
        "  nodes = update_node_fn(nodes)\n",
        "\n",
        "  # 2. Aggregate node features over nodes in neighborhood\n",
        "  # Equivalent to jnp.sum(n_node), but jittable\n",
        "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "  aggregate_nodes_fn = jax.ops.segment_sum\n",
        "\n",
        "  # Compute new node features by aggregating messages from neighboring nodes\n",
        "  nodes = tree.tree_map(lambda x: aggregate_nodes_fn(x[senders], receivers,\n",
        "                                        total_num_nodes), nodes)\n",
        "  out_graph = graph._replace(nodes=nodes)\n",
        "  return out_graph\n",
        "\n",
        "def add_self_edges_fn(receivers, senders, total_num_nodes):\n",
        "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
        "  receivers = jnp.concatenate((receivers, jnp.arange(total_num_nodes)), axis=0)\n",
        "  senders = jnp.concatenate((senders, jnp.arange(total_num_nodes)), axis=0)\n",
        "  return receivers, senders\n",
        "\n",
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
        "def GraphConvolution(\n",
        "    update_node_fn,\n",
        "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
        "    add_self_edges: bool = False,\n",
        "    symmetric_normalization: bool = True):\n",
        "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
        "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
        "  NOTE: This implementation does not add an activation after aggregation.\n",
        "  If you are stacking layers, you may want to add an activation between\n",
        "  each layer.\n",
        "  Args:\n",
        "    update_node_fn: function used to update the nodes. In the paper a single\n",
        "      layer MLP is used.\n",
        "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
        "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
        "      paper definition of GCN. Defaults to False.\n",
        "    symmetric_normalization: whether to use symmetric normalization. Defaults\n",
        "      to True.\n",
        "  Returns:\n",
        "    A method that applies a Graph Convolution layer.\n",
        "  \"\"\"\n",
        "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
        "    nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "    # First pass nodes through the node updater.\n",
        "    nodes = update_node_fn(nodes)\n",
        "    # Equivalent to jnp.sum(n_node), but jittable\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
        "      # this case it is not required since a GCN is agnostic to whether\n",
        "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
        "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders, total_num_nodes)\n",
        "    else:\n",
        "      conv_senders = senders\n",
        "      conv_receivers = receivers\n",
        "\n",
        "    # pylint: disable=g-long-lambda\n",
        "    if symmetric_normalization:\n",
        "      # Calculate the normalization values.\n",
        "      count_edges = lambda x: jax.ops.segment_sum(\n",
        "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
        "      sender_degree = count_edges(conv_senders)\n",
        "      receiver_degree = count_edges(conv_receivers)\n",
        "\n",
        "      # Pre normalize by sqrt sender degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
        "          nodes,\n",
        "      )\n",
        "      # Aggregate the pre-normalized nodes.\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "      # Post normalize by sqrt receiver degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x:\n",
        "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
        "          nodes,\n",
        "      )\n",
        "    else:\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "    # pylint: enable=g-long-lambda\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  return _ApplyGCN\n",
        "\n",
        "def gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(64)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(32)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=hk.Linear(7))\n",
        "  graph = gn(graph)\n",
        "  return graph\n",
        "\n",
        "def gcn_definition(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Defines a GCN for the Cora dataset task.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(64)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=hk.Linear(7)) # output dim is 7 because we have 7 output classes.\n",
        "  graph = gn(graph)\n",
        "  return graph"
      ],
      "metadata": {
        "id": "OvGVFILU0iHG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_cora_dataset(network, num_steps: int):\n",
        "  \"\"\"Solves the Cora dataset problem by optimizing.\"\"\"\n",
        "  cora_dataset = get_cora_dataset()\n",
        "  labels = get_ground_truth_assignments_for_cora_dataset()\n",
        "  params = network.init(jax.random.PRNGKey(42), cora_dataset)\n",
        "\n",
        "  @jax.jit\n",
        "  def predict(params):\n",
        "    decoded_graph = network.apply(params, cora_dataset)\n",
        "    return jnp.argmax(decoded_graph.nodes, axis=1)\n",
        "\n",
        "  @jax.jit\n",
        "  def prediction_loss(params):\n",
        "    decoded_graph = network.apply(params, cora_dataset)\n",
        "    # We interpret the decoded nodes as a pair of logits for each node.\n",
        "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
        "    # Here I have given some aprioric knowledge to the training set. I have\n",
        "    # used 2166 randomly shuffled samples to predict whole labels in the dataset\n",
        "    sonuc = 0\n",
        "    iterate = np.arange(2708)\n",
        "    np.random.shuffle(iterate)\n",
        "    for i in iterate[:2166]:\n",
        "      j = classes[i][1]\n",
        "      sonuc += log_prob[i,j]\n",
        "    return -(sonuc)\n",
        "\n",
        "  opt_init, opt_update = optax.adam(1e-2)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  @jax.jit\n",
        "  def update(params, opt_state):\n",
        "    g = jax.grad(prediction_loss)(params)\n",
        "    updates, opt_state = opt_update(g, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state\n",
        "\n",
        "  @jax.jit\n",
        "  def accuracy(params):\n",
        "    decoded_graph = network.apply(params, cora_dataset)\n",
        "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
        "\n",
        "  for step in range(num_steps):\n",
        "    print(f\"step {step} accuracy {accuracy(params).item():.4f}\")\n",
        "    params, opt_state = update(params, opt_state)\n",
        "\n",
        "  return predict(params)"
      ],
      "metadata": {
        "id": "Hv6dN0JZ0oef"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = hk.without_apply_rng(hk.transform(gcn_definition))\n",
        "result = optimize_cora_dataset(network, num_steps=500)\n",
        "\n",
        "# accuracy : %96.38 for 2166 samples after 500 epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoRA_IO0u0G",
        "outputId": "aabafd74-78b6-484e-a76e-b1cbeff3f3d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 accuracy 0.1208\n",
            "step 1 accuracy 0.5853\n",
            "step 2 accuracy 0.5853\n",
            "step 3 accuracy 0.5842\n",
            "step 4 accuracy 0.5835\n",
            "step 5 accuracy 0.5831\n",
            "step 6 accuracy 0.5890\n",
            "step 7 accuracy 0.6126\n",
            "step 8 accuracy 0.6536\n",
            "step 9 accuracy 0.6983\n",
            "step 10 accuracy 0.7194\n",
            "step 11 accuracy 0.7397\n",
            "step 12 accuracy 0.7703\n",
            "step 13 accuracy 0.7973\n",
            "step 14 accuracy 0.8257\n",
            "step 15 accuracy 0.8497\n",
            "step 16 accuracy 0.8652\n",
            "step 17 accuracy 0.8733\n",
            "step 18 accuracy 0.8818\n",
            "step 19 accuracy 0.8914\n",
            "step 20 accuracy 0.8970\n",
            "step 21 accuracy 0.9014\n",
            "step 22 accuracy 0.9036\n",
            "step 23 accuracy 0.9055\n",
            "step 24 accuracy 0.9077\n",
            "step 25 accuracy 0.9117\n",
            "step 26 accuracy 0.9129\n",
            "step 27 accuracy 0.9140\n",
            "step 28 accuracy 0.9165\n",
            "step 29 accuracy 0.9191\n",
            "step 30 accuracy 0.9180\n",
            "step 31 accuracy 0.9191\n",
            "step 32 accuracy 0.9202\n",
            "step 33 accuracy 0.9217\n",
            "step 34 accuracy 0.9221\n",
            "step 35 accuracy 0.9236\n",
            "step 36 accuracy 0.9247\n",
            "step 37 accuracy 0.9261\n",
            "step 38 accuracy 0.9273\n",
            "step 39 accuracy 0.9306\n",
            "step 40 accuracy 0.9335\n",
            "step 41 accuracy 0.9346\n",
            "step 42 accuracy 0.9354\n",
            "step 43 accuracy 0.9365\n",
            "step 44 accuracy 0.9369\n",
            "step 45 accuracy 0.9391\n",
            "step 46 accuracy 0.9394\n",
            "step 47 accuracy 0.9405\n",
            "step 48 accuracy 0.9420\n",
            "step 49 accuracy 0.9420\n",
            "step 50 accuracy 0.9424\n",
            "step 51 accuracy 0.9435\n",
            "step 52 accuracy 0.9442\n",
            "step 53 accuracy 0.9461\n",
            "step 54 accuracy 0.9479\n",
            "step 55 accuracy 0.9479\n",
            "step 56 accuracy 0.9487\n",
            "step 57 accuracy 0.9494\n",
            "step 58 accuracy 0.9509\n",
            "step 59 accuracy 0.9509\n",
            "step 60 accuracy 0.9516\n",
            "step 61 accuracy 0.9524\n",
            "step 62 accuracy 0.9538\n",
            "step 63 accuracy 0.9542\n",
            "step 64 accuracy 0.9557\n",
            "step 65 accuracy 0.9564\n",
            "step 66 accuracy 0.9561\n",
            "step 67 accuracy 0.9561\n",
            "step 68 accuracy 0.9557\n",
            "step 69 accuracy 0.9557\n",
            "step 70 accuracy 0.9553\n",
            "step 71 accuracy 0.9553\n",
            "step 72 accuracy 0.9557\n",
            "step 73 accuracy 0.9561\n",
            "step 74 accuracy 0.9568\n",
            "step 75 accuracy 0.9568\n",
            "step 76 accuracy 0.9568\n",
            "step 77 accuracy 0.9568\n",
            "step 78 accuracy 0.9568\n",
            "step 79 accuracy 0.9568\n",
            "step 80 accuracy 0.9568\n",
            "step 81 accuracy 0.9572\n",
            "step 82 accuracy 0.9572\n",
            "step 83 accuracy 0.9586\n",
            "step 84 accuracy 0.9586\n",
            "step 85 accuracy 0.9590\n",
            "step 86 accuracy 0.9594\n",
            "step 87 accuracy 0.9594\n",
            "step 88 accuracy 0.9597\n",
            "step 89 accuracy 0.9601\n",
            "step 90 accuracy 0.9605\n",
            "step 91 accuracy 0.9605\n",
            "step 92 accuracy 0.9605\n",
            "step 93 accuracy 0.9605\n",
            "step 94 accuracy 0.9609\n",
            "step 95 accuracy 0.9609\n",
            "step 96 accuracy 0.9609\n",
            "step 97 accuracy 0.9609\n",
            "step 98 accuracy 0.9609\n",
            "step 99 accuracy 0.9616\n",
            "step 100 accuracy 0.9612\n",
            "step 101 accuracy 0.9612\n",
            "step 102 accuracy 0.9612\n",
            "step 103 accuracy 0.9616\n",
            "step 104 accuracy 0.9620\n",
            "step 105 accuracy 0.9623\n",
            "step 106 accuracy 0.9627\n",
            "step 107 accuracy 0.9631\n",
            "step 108 accuracy 0.9631\n",
            "step 109 accuracy 0.9634\n",
            "step 110 accuracy 0.9634\n",
            "step 111 accuracy 0.9634\n",
            "step 112 accuracy 0.9634\n",
            "step 113 accuracy 0.9634\n",
            "step 114 accuracy 0.9631\n",
            "step 115 accuracy 0.9631\n",
            "step 116 accuracy 0.9631\n",
            "step 117 accuracy 0.9631\n",
            "step 118 accuracy 0.9627\n",
            "step 119 accuracy 0.9627\n",
            "step 120 accuracy 0.9627\n",
            "step 121 accuracy 0.9627\n",
            "step 122 accuracy 0.9631\n",
            "step 123 accuracy 0.9631\n",
            "step 124 accuracy 0.9631\n",
            "step 125 accuracy 0.9631\n",
            "step 126 accuracy 0.9631\n",
            "step 127 accuracy 0.9631\n",
            "step 128 accuracy 0.9631\n",
            "step 129 accuracy 0.9631\n",
            "step 130 accuracy 0.9631\n",
            "step 131 accuracy 0.9631\n",
            "step 132 accuracy 0.9631\n",
            "step 133 accuracy 0.9634\n",
            "step 134 accuracy 0.9634\n",
            "step 135 accuracy 0.9634\n",
            "step 136 accuracy 0.9634\n",
            "step 137 accuracy 0.9634\n",
            "step 138 accuracy 0.9634\n",
            "step 139 accuracy 0.9638\n",
            "step 140 accuracy 0.9638\n",
            "step 141 accuracy 0.9638\n",
            "step 142 accuracy 0.9638\n",
            "step 143 accuracy 0.9638\n",
            "step 144 accuracy 0.9638\n",
            "step 145 accuracy 0.9631\n",
            "step 146 accuracy 0.9631\n",
            "step 147 accuracy 0.9631\n",
            "step 148 accuracy 0.9631\n",
            "step 149 accuracy 0.9631\n",
            "step 150 accuracy 0.9631\n",
            "step 151 accuracy 0.9631\n",
            "step 152 accuracy 0.9631\n",
            "step 153 accuracy 0.9631\n",
            "step 154 accuracy 0.9631\n",
            "step 155 accuracy 0.9631\n",
            "step 156 accuracy 0.9631\n",
            "step 157 accuracy 0.9631\n",
            "step 158 accuracy 0.9631\n",
            "step 159 accuracy 0.9631\n",
            "step 160 accuracy 0.9631\n",
            "step 161 accuracy 0.9627\n",
            "step 162 accuracy 0.9627\n",
            "step 163 accuracy 0.9627\n",
            "step 164 accuracy 0.9631\n",
            "step 165 accuracy 0.9631\n",
            "step 166 accuracy 0.9631\n",
            "step 167 accuracy 0.9634\n",
            "step 168 accuracy 0.9634\n",
            "step 169 accuracy 0.9638\n",
            "step 170 accuracy 0.9638\n",
            "step 171 accuracy 0.9638\n",
            "step 172 accuracy 0.9642\n",
            "step 173 accuracy 0.9642\n",
            "step 174 accuracy 0.9638\n",
            "step 175 accuracy 0.9638\n",
            "step 176 accuracy 0.9638\n",
            "step 177 accuracy 0.9638\n",
            "step 178 accuracy 0.9634\n",
            "step 179 accuracy 0.9634\n",
            "step 180 accuracy 0.9634\n",
            "step 181 accuracy 0.9634\n",
            "step 182 accuracy 0.9634\n",
            "step 183 accuracy 0.9634\n",
            "step 184 accuracy 0.9634\n",
            "step 185 accuracy 0.9634\n",
            "step 186 accuracy 0.9634\n",
            "step 187 accuracy 0.9634\n",
            "step 188 accuracy 0.9634\n",
            "step 189 accuracy 0.9631\n",
            "step 190 accuracy 0.9631\n",
            "step 191 accuracy 0.9631\n",
            "step 192 accuracy 0.9631\n",
            "step 193 accuracy 0.9631\n",
            "step 194 accuracy 0.9638\n",
            "step 195 accuracy 0.9638\n",
            "step 196 accuracy 0.9638\n",
            "step 197 accuracy 0.9638\n",
            "step 198 accuracy 0.9638\n",
            "step 199 accuracy 0.9638\n",
            "step 200 accuracy 0.9638\n",
            "step 201 accuracy 0.9634\n",
            "step 202 accuracy 0.9634\n",
            "step 203 accuracy 0.9634\n",
            "step 204 accuracy 0.9634\n",
            "step 205 accuracy 0.9634\n",
            "step 206 accuracy 0.9634\n",
            "step 207 accuracy 0.9634\n",
            "step 208 accuracy 0.9634\n",
            "step 209 accuracy 0.9634\n",
            "step 210 accuracy 0.9634\n",
            "step 211 accuracy 0.9631\n",
            "step 212 accuracy 0.9631\n",
            "step 213 accuracy 0.9631\n",
            "step 214 accuracy 0.9631\n",
            "step 215 accuracy 0.9631\n",
            "step 216 accuracy 0.9623\n",
            "step 217 accuracy 0.9623\n",
            "step 218 accuracy 0.9623\n",
            "step 219 accuracy 0.9623\n",
            "step 220 accuracy 0.9623\n",
            "step 221 accuracy 0.9623\n",
            "step 222 accuracy 0.9623\n",
            "step 223 accuracy 0.9623\n",
            "step 224 accuracy 0.9623\n",
            "step 225 accuracy 0.9623\n",
            "step 226 accuracy 0.9623\n",
            "step 227 accuracy 0.9623\n",
            "step 228 accuracy 0.9623\n",
            "step 229 accuracy 0.9623\n",
            "step 230 accuracy 0.9620\n",
            "step 231 accuracy 0.9620\n",
            "step 232 accuracy 0.9620\n",
            "step 233 accuracy 0.9620\n",
            "step 234 accuracy 0.9620\n",
            "step 235 accuracy 0.9620\n",
            "step 236 accuracy 0.9620\n",
            "step 237 accuracy 0.9620\n",
            "step 238 accuracy 0.9620\n",
            "step 239 accuracy 0.9620\n",
            "step 240 accuracy 0.9620\n",
            "step 241 accuracy 0.9616\n",
            "step 242 accuracy 0.9616\n",
            "step 243 accuracy 0.9616\n",
            "step 244 accuracy 0.9616\n",
            "step 245 accuracy 0.9612\n",
            "step 246 accuracy 0.9612\n",
            "step 247 accuracy 0.9612\n",
            "step 248 accuracy 0.9609\n",
            "step 249 accuracy 0.9609\n",
            "step 250 accuracy 0.9609\n",
            "step 251 accuracy 0.9609\n",
            "step 252 accuracy 0.9609\n",
            "step 253 accuracy 0.9609\n",
            "step 254 accuracy 0.9605\n",
            "step 255 accuracy 0.9605\n",
            "step 256 accuracy 0.9605\n",
            "step 257 accuracy 0.9605\n",
            "step 258 accuracy 0.9605\n",
            "step 259 accuracy 0.9605\n",
            "step 260 accuracy 0.9605\n",
            "step 261 accuracy 0.9605\n",
            "step 262 accuracy 0.9605\n",
            "step 263 accuracy 0.9605\n",
            "step 264 accuracy 0.9609\n",
            "step 265 accuracy 0.9609\n",
            "step 266 accuracy 0.9609\n",
            "step 267 accuracy 0.9609\n",
            "step 268 accuracy 0.9609\n",
            "step 269 accuracy 0.9609\n",
            "step 270 accuracy 0.9609\n",
            "step 271 accuracy 0.9609\n",
            "step 272 accuracy 0.9609\n",
            "step 273 accuracy 0.9609\n",
            "step 274 accuracy 0.9609\n",
            "step 275 accuracy 0.9609\n",
            "step 276 accuracy 0.9609\n",
            "step 277 accuracy 0.9609\n",
            "step 278 accuracy 0.9609\n",
            "step 279 accuracy 0.9609\n",
            "step 280 accuracy 0.9609\n",
            "step 281 accuracy 0.9609\n",
            "step 282 accuracy 0.9609\n",
            "step 283 accuracy 0.9609\n",
            "step 284 accuracy 0.9609\n",
            "step 285 accuracy 0.9609\n",
            "step 286 accuracy 0.9609\n",
            "step 287 accuracy 0.9609\n",
            "step 288 accuracy 0.9609\n",
            "step 289 accuracy 0.9609\n",
            "step 290 accuracy 0.9609\n",
            "step 291 accuracy 0.9609\n",
            "step 292 accuracy 0.9609\n",
            "step 293 accuracy 0.9609\n",
            "step 294 accuracy 0.9609\n",
            "step 295 accuracy 0.9609\n",
            "step 296 accuracy 0.9609\n",
            "step 297 accuracy 0.9609\n",
            "step 298 accuracy 0.9609\n",
            "step 299 accuracy 0.9609\n",
            "step 300 accuracy 0.9609\n",
            "step 301 accuracy 0.9609\n",
            "step 302 accuracy 0.9609\n",
            "step 303 accuracy 0.9609\n",
            "step 304 accuracy 0.9609\n",
            "step 305 accuracy 0.9609\n",
            "step 306 accuracy 0.9605\n",
            "step 307 accuracy 0.9605\n",
            "step 308 accuracy 0.9605\n",
            "step 309 accuracy 0.9605\n",
            "step 310 accuracy 0.9605\n",
            "step 311 accuracy 0.9605\n",
            "step 312 accuracy 0.9605\n",
            "step 313 accuracy 0.9605\n",
            "step 314 accuracy 0.9601\n",
            "step 315 accuracy 0.9601\n",
            "step 316 accuracy 0.9601\n",
            "step 317 accuracy 0.9601\n",
            "step 318 accuracy 0.9601\n",
            "step 319 accuracy 0.9601\n",
            "step 320 accuracy 0.9601\n",
            "step 321 accuracy 0.9601\n",
            "step 322 accuracy 0.9601\n",
            "step 323 accuracy 0.9601\n",
            "step 324 accuracy 0.9601\n",
            "step 325 accuracy 0.9601\n",
            "step 326 accuracy 0.9601\n",
            "step 327 accuracy 0.9601\n",
            "step 328 accuracy 0.9601\n",
            "step 329 accuracy 0.9601\n",
            "step 330 accuracy 0.9601\n",
            "step 331 accuracy 0.9601\n",
            "step 332 accuracy 0.9601\n",
            "step 333 accuracy 0.9601\n",
            "step 334 accuracy 0.9601\n",
            "step 335 accuracy 0.9601\n",
            "step 336 accuracy 0.9601\n",
            "step 337 accuracy 0.9601\n",
            "step 338 accuracy 0.9601\n",
            "step 339 accuracy 0.9601\n",
            "step 340 accuracy 0.9601\n",
            "step 341 accuracy 0.9601\n",
            "step 342 accuracy 0.9601\n",
            "step 343 accuracy 0.9601\n",
            "step 344 accuracy 0.9601\n",
            "step 345 accuracy 0.9601\n",
            "step 346 accuracy 0.9601\n",
            "step 347 accuracy 0.9601\n",
            "step 348 accuracy 0.9601\n",
            "step 349 accuracy 0.9601\n",
            "step 350 accuracy 0.9601\n",
            "step 351 accuracy 0.9601\n",
            "step 352 accuracy 0.9601\n",
            "step 353 accuracy 0.9601\n",
            "step 354 accuracy 0.9601\n",
            "step 355 accuracy 0.9601\n",
            "step 356 accuracy 0.9601\n",
            "step 357 accuracy 0.9601\n",
            "step 358 accuracy 0.9601\n",
            "step 359 accuracy 0.9601\n",
            "step 360 accuracy 0.9601\n",
            "step 361 accuracy 0.9601\n",
            "step 362 accuracy 0.9601\n",
            "step 363 accuracy 0.9601\n",
            "step 364 accuracy 0.9601\n",
            "step 365 accuracy 0.9601\n",
            "step 366 accuracy 0.9601\n",
            "step 367 accuracy 0.9601\n",
            "step 368 accuracy 0.9601\n",
            "step 369 accuracy 0.9601\n",
            "step 370 accuracy 0.9601\n",
            "step 371 accuracy 0.9601\n",
            "step 372 accuracy 0.9601\n",
            "step 373 accuracy 0.9601\n",
            "step 374 accuracy 0.9601\n",
            "step 375 accuracy 0.9601\n",
            "step 376 accuracy 0.9601\n",
            "step 377 accuracy 0.9601\n",
            "step 378 accuracy 0.9601\n",
            "step 379 accuracy 0.9601\n",
            "step 380 accuracy 0.9601\n",
            "step 381 accuracy 0.9601\n",
            "step 382 accuracy 0.9601\n",
            "step 383 accuracy 0.9601\n",
            "step 384 accuracy 0.9601\n",
            "step 385 accuracy 0.9601\n",
            "step 386 accuracy 0.9601\n",
            "step 387 accuracy 0.9601\n",
            "step 388 accuracy 0.9601\n",
            "step 389 accuracy 0.9601\n",
            "step 390 accuracy 0.9601\n",
            "step 391 accuracy 0.9601\n",
            "step 392 accuracy 0.9601\n",
            "step 393 accuracy 0.9601\n",
            "step 394 accuracy 0.9601\n",
            "step 395 accuracy 0.9601\n",
            "step 396 accuracy 0.9601\n",
            "step 397 accuracy 0.9601\n",
            "step 398 accuracy 0.9601\n",
            "step 399 accuracy 0.9601\n",
            "step 400 accuracy 0.9601\n",
            "step 401 accuracy 0.9601\n",
            "step 402 accuracy 0.9601\n",
            "step 403 accuracy 0.9601\n",
            "step 404 accuracy 0.9601\n",
            "step 405 accuracy 0.9601\n",
            "step 406 accuracy 0.9601\n",
            "step 407 accuracy 0.9601\n",
            "step 408 accuracy 0.9601\n",
            "step 409 accuracy 0.9601\n",
            "step 410 accuracy 0.9601\n",
            "step 411 accuracy 0.9601\n",
            "step 412 accuracy 0.9601\n",
            "step 413 accuracy 0.9601\n",
            "step 414 accuracy 0.9601\n",
            "step 415 accuracy 0.9601\n",
            "step 416 accuracy 0.9601\n",
            "step 417 accuracy 0.9601\n",
            "step 418 accuracy 0.9601\n",
            "step 419 accuracy 0.9601\n",
            "step 420 accuracy 0.9601\n",
            "step 421 accuracy 0.9601\n",
            "step 422 accuracy 0.9601\n",
            "step 423 accuracy 0.9601\n",
            "step 424 accuracy 0.9601\n",
            "step 425 accuracy 0.9601\n",
            "step 426 accuracy 0.9601\n",
            "step 427 accuracy 0.9601\n",
            "step 428 accuracy 0.9601\n",
            "step 429 accuracy 0.9601\n",
            "step 430 accuracy 0.9601\n",
            "step 431 accuracy 0.9601\n",
            "step 432 accuracy 0.9601\n",
            "step 433 accuracy 0.9601\n",
            "step 434 accuracy 0.9601\n",
            "step 435 accuracy 0.9601\n",
            "step 436 accuracy 0.9601\n",
            "step 437 accuracy 0.9601\n",
            "step 438 accuracy 0.9601\n",
            "step 439 accuracy 0.9601\n",
            "step 440 accuracy 0.9601\n",
            "step 441 accuracy 0.9601\n",
            "step 442 accuracy 0.9601\n",
            "step 443 accuracy 0.9601\n",
            "step 444 accuracy 0.9601\n",
            "step 445 accuracy 0.9601\n",
            "step 446 accuracy 0.9601\n",
            "step 447 accuracy 0.9601\n",
            "step 448 accuracy 0.9601\n",
            "step 449 accuracy 0.9601\n",
            "step 450 accuracy 0.9601\n",
            "step 451 accuracy 0.9601\n",
            "step 452 accuracy 0.9601\n",
            "step 453 accuracy 0.9601\n",
            "step 454 accuracy 0.9601\n",
            "step 455 accuracy 0.9601\n",
            "step 456 accuracy 0.9601\n",
            "step 457 accuracy 0.9601\n",
            "step 458 accuracy 0.9601\n",
            "step 459 accuracy 0.9601\n",
            "step 460 accuracy 0.9601\n",
            "step 461 accuracy 0.9601\n",
            "step 462 accuracy 0.9601\n",
            "step 463 accuracy 0.9601\n",
            "step 464 accuracy 0.9601\n",
            "step 465 accuracy 0.9601\n",
            "step 466 accuracy 0.9601\n",
            "step 467 accuracy 0.9601\n",
            "step 468 accuracy 0.9601\n",
            "step 469 accuracy 0.9601\n",
            "step 470 accuracy 0.9601\n",
            "step 471 accuracy 0.9601\n",
            "step 472 accuracy 0.9601\n",
            "step 473 accuracy 0.9601\n",
            "step 474 accuracy 0.9601\n",
            "step 475 accuracy 0.9601\n",
            "step 476 accuracy 0.9601\n",
            "step 477 accuracy 0.9601\n",
            "step 478 accuracy 0.9601\n",
            "step 479 accuracy 0.9601\n",
            "step 480 accuracy 0.9605\n",
            "step 481 accuracy 0.9605\n",
            "step 482 accuracy 0.9605\n",
            "step 483 accuracy 0.9605\n",
            "step 484 accuracy 0.9605\n",
            "step 485 accuracy 0.9605\n",
            "step 486 accuracy 0.9605\n",
            "step 487 accuracy 0.9605\n",
            "step 488 accuracy 0.9605\n",
            "step 489 accuracy 0.9605\n",
            "step 490 accuracy 0.9605\n",
            "step 491 accuracy 0.9605\n",
            "step 492 accuracy 0.9605\n",
            "step 493 accuracy 0.9605\n",
            "step 494 accuracy 0.9605\n",
            "step 495 accuracy 0.9605\n",
            "step 496 accuracy 0.9605\n",
            "step 497 accuracy 0.9605\n",
            "step 498 accuracy 0.9605\n",
            "step 499 accuracy 0.9605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### GAT Implementation #########\n",
        "\n",
        "# GAT implementation adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L442.\n",
        "def GAT(attention_query_fn,\n",
        "        attention_logit_fn,\n",
        "        node_update_fn=None,\n",
        "        add_self_edges=True):\n",
        "  \"\"\"Returns a method that applies a Graph Attention Network layer.\n",
        "  Graph Attention message passing as described in\n",
        "  https://arxiv.org/pdf/1710.10903.pdf. This model expects node features as a\n",
        "  jnp.array, may use edge features for computing attention weights, and\n",
        "  ignore global features. It does not support nests.\n",
        "  Args:\n",
        "    attention_query_fn: function that generates attention queries\n",
        "      from sender node features.\n",
        "    attention_logit_fn: function that converts attention queries into logits for\n",
        "      softmax attention.\n",
        "    node_update_fn: function that updates the aggregated messages. If None,\n",
        "      will apply leaky relu and concatenate (if using multi-head attention).\n",
        "  Returns:\n",
        "    A function that applies a Graph Attention layer.\n",
        "  \"\"\"\n",
        "  # pylint: disable=g-long-lambda\n",
        "  if node_update_fn is None:\n",
        "    # By default, apply the leaky relu and then concatenate the heads on the\n",
        "    # feature axis.\n",
        "    node_update_fn = lambda x: jnp.reshape(\n",
        "        jax.nn.leaky_relu(x), (x.shape[0], -1))\n",
        "\n",
        "  def _ApplyGAT(graph):\n",
        "    \"\"\"Applies a Graph Attention layer.\"\"\"\n",
        "    nodes, edges, receivers, senders, _, _, _ = graph\n",
        "    # Equivalent to the sum of n_node, but statically known.\n",
        "    try:\n",
        "      sum_n_node = nodes.shape[0]\n",
        "    except IndexError:\n",
        "      raise IndexError('GAT requires node features')\n",
        "\n",
        "    # Pass nodes through the attention query function to transform\n",
        "    # node features, e.g. with an MLP.\n",
        "    nodes = attention_query_fn(nodes)\n",
        "\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      receivers, senders = add_self_edges_fn(receivers, senders, total_num_nodes)\n",
        "\n",
        "    # We compute the softmax logits using a function that takes the\n",
        "    # embedded sender and receiver attributes.\n",
        "    sent_attributes = nodes[senders]\n",
        "    received_attributes = nodes[receivers]\n",
        "    att_softmax_logits = attention_logit_fn(\n",
        "        sent_attributes, received_attributes, edges)\n",
        "\n",
        "    # Compute the attention softmax weights on the entire tree.\n",
        "    att_weights = jraph.segment_softmax(att_softmax_logits, segment_ids=receivers,\n",
        "                                    num_segments=sum_n_node)\n",
        "\n",
        "    # Apply attention weights.\n",
        "    messages = sent_attributes * att_weights\n",
        "    # Aggregate messages to nodes.\n",
        "    nodes = jax.ops.segment_sum(messages, receivers, num_segments=sum_n_node)\n",
        "\n",
        "    # Apply an update function to the aggregated messages.\n",
        "    nodes = node_update_fn(nodes)\n",
        "\n",
        "    return graph._replace(nodes=nodes)\n",
        "  # pylint: enable=g-long-lambda\n",
        "  return _ApplyGAT"
      ],
      "metadata": {
        "id": "GkcvfmM5SdkF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_logit_fn(sender_attr, receiver_attr, edges):\n",
        "  del edges\n",
        "  x = jnp.concatenate((sender_attr, receiver_attr), axis=1)\n",
        "  return hk.Linear(1)(x)\n",
        "\n",
        "gat_layer = GAT(\n",
        "    attention_query_fn=lambda n: hk.Linear(7)(n),  # Applies W to the node features\n",
        "    attention_logit_fn=attention_logit_fn,\n",
        "    node_update_fn=None,\n",
        "    add_self_edges=True,\n",
        ")"
      ],
      "metadata": {
        "id": "P9qOjv1vSmB9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gat_definition(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Defines a GAT network for the Cora dataset node classification task.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  def _attention_logit_fn( sender_attr, receiver_attr, edges):\n",
        "    del edges\n",
        "    x = jnp.concatenate((sender_attr, receiver_attr), axis=1)\n",
        "    return hk.Linear(1)(x)\n",
        "\n",
        "  gn = GAT(\n",
        "    attention_query_fn=lambda n: hk.Linear(64)(n),\n",
        "    attention_logit_fn=_attention_logit_fn,\n",
        "    node_update_fn=None,\n",
        "    add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GAT(\n",
        "    attention_query_fn=lambda n: hk.Linear(32)(n),\n",
        "    attention_logit_fn=_attention_logit_fn,\n",
        "    node_update_fn=hk.Linear(7),\n",
        "    add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "  return graph"
      ],
      "metadata": {
        "id": "yA0v9NOmSoc0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = hk.without_apply_rng(hk.transform(gat_definition))\n",
        "result = optimize_cora_dataset(network, num_steps=500)\n",
        "# best accuracy : %96.94 for 2166 samples after 500 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8x1zphjSreP",
        "outputId": "8a20a47f-9305-4176-b39b-9e14155b3b6d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 accuracy 0.1030\n",
            "step 1 accuracy 0.3021\n",
            "step 2 accuracy 0.3021\n",
            "step 3 accuracy 0.3021\n",
            "step 4 accuracy 0.3021\n",
            "step 5 accuracy 0.3021\n",
            "step 6 accuracy 0.3021\n",
            "step 7 accuracy 0.3021\n",
            "step 8 accuracy 0.3220\n",
            "step 9 accuracy 0.5391\n",
            "step 10 accuracy 0.6547\n",
            "step 11 accuracy 0.8058\n",
            "step 12 accuracy 0.8386\n",
            "step 13 accuracy 0.8578\n",
            "step 14 accuracy 0.9007\n",
            "step 15 accuracy 0.9062\n",
            "step 16 accuracy 0.9129\n",
            "step 17 accuracy 0.9147\n",
            "step 18 accuracy 0.9151\n",
            "step 19 accuracy 0.9213\n",
            "step 20 accuracy 0.9243\n",
            "step 21 accuracy 0.9280\n",
            "step 22 accuracy 0.9298\n",
            "step 23 accuracy 0.9346\n",
            "step 24 accuracy 0.9361\n",
            "step 25 accuracy 0.9387\n",
            "step 26 accuracy 0.9402\n",
            "step 27 accuracy 0.9428\n",
            "step 28 accuracy 0.9450\n",
            "step 29 accuracy 0.9479\n",
            "step 30 accuracy 0.9494\n",
            "step 31 accuracy 0.9494\n",
            "step 32 accuracy 0.9542\n",
            "step 33 accuracy 0.9583\n",
            "step 34 accuracy 0.9579\n",
            "step 35 accuracy 0.9583\n",
            "step 36 accuracy 0.9579\n",
            "step 37 accuracy 0.9594\n",
            "step 38 accuracy 0.9601\n",
            "step 39 accuracy 0.9631\n",
            "step 40 accuracy 0.9627\n",
            "step 41 accuracy 0.9631\n",
            "step 42 accuracy 0.9653\n",
            "step 43 accuracy 0.9660\n",
            "step 44 accuracy 0.9668\n",
            "step 45 accuracy 0.9668\n",
            "step 46 accuracy 0.9664\n",
            "step 47 accuracy 0.9679\n",
            "step 48 accuracy 0.9679\n",
            "step 49 accuracy 0.9679\n",
            "step 50 accuracy 0.9675\n",
            "step 51 accuracy 0.9671\n",
            "step 52 accuracy 0.9679\n",
            "step 53 accuracy 0.9675\n",
            "step 54 accuracy 0.9675\n",
            "step 55 accuracy 0.9671\n",
            "step 56 accuracy 0.9671\n",
            "step 57 accuracy 0.9671\n",
            "step 58 accuracy 0.9675\n",
            "step 59 accuracy 0.9686\n",
            "step 60 accuracy 0.9686\n",
            "step 61 accuracy 0.9690\n",
            "step 62 accuracy 0.9690\n",
            "step 63 accuracy 0.9690\n",
            "step 64 accuracy 0.9690\n",
            "step 65 accuracy 0.9690\n",
            "step 66 accuracy 0.9690\n",
            "step 67 accuracy 0.9686\n",
            "step 68 accuracy 0.9682\n",
            "step 69 accuracy 0.9682\n",
            "step 70 accuracy 0.9682\n",
            "step 71 accuracy 0.9682\n",
            "step 72 accuracy 0.9682\n",
            "step 73 accuracy 0.9682\n",
            "step 74 accuracy 0.9682\n",
            "step 75 accuracy 0.9682\n",
            "step 76 accuracy 0.9682\n",
            "step 77 accuracy 0.9682\n",
            "step 78 accuracy 0.9686\n",
            "step 79 accuracy 0.9682\n",
            "step 80 accuracy 0.9682\n",
            "step 81 accuracy 0.9679\n",
            "step 82 accuracy 0.9679\n",
            "step 83 accuracy 0.9679\n",
            "step 84 accuracy 0.9679\n",
            "step 85 accuracy 0.9679\n",
            "step 86 accuracy 0.9679\n",
            "step 87 accuracy 0.9682\n",
            "step 88 accuracy 0.9679\n",
            "step 89 accuracy 0.9679\n",
            "step 90 accuracy 0.9679\n",
            "step 91 accuracy 0.9682\n",
            "step 92 accuracy 0.9679\n",
            "step 93 accuracy 0.9679\n",
            "step 94 accuracy 0.9679\n",
            "step 95 accuracy 0.9682\n",
            "step 96 accuracy 0.9682\n",
            "step 97 accuracy 0.9682\n",
            "step 98 accuracy 0.9682\n",
            "step 99 accuracy 0.9682\n",
            "step 100 accuracy 0.9682\n",
            "step 101 accuracy 0.9682\n",
            "step 102 accuracy 0.9682\n",
            "step 103 accuracy 0.9682\n",
            "step 104 accuracy 0.9682\n",
            "step 105 accuracy 0.9682\n",
            "step 106 accuracy 0.9682\n",
            "step 107 accuracy 0.9682\n",
            "step 108 accuracy 0.9682\n",
            "step 109 accuracy 0.9682\n",
            "step 110 accuracy 0.9682\n",
            "step 111 accuracy 0.9682\n",
            "step 112 accuracy 0.9682\n",
            "step 113 accuracy 0.9682\n",
            "step 114 accuracy 0.9682\n",
            "step 115 accuracy 0.9682\n",
            "step 116 accuracy 0.9682\n",
            "step 117 accuracy 0.9682\n",
            "step 118 accuracy 0.9682\n",
            "step 119 accuracy 0.9682\n",
            "step 120 accuracy 0.9682\n",
            "step 121 accuracy 0.9682\n",
            "step 122 accuracy 0.9682\n",
            "step 123 accuracy 0.9682\n",
            "step 124 accuracy 0.9682\n",
            "step 125 accuracy 0.9682\n",
            "step 126 accuracy 0.9682\n",
            "step 127 accuracy 0.9682\n",
            "step 128 accuracy 0.9682\n",
            "step 129 accuracy 0.9682\n",
            "step 130 accuracy 0.9679\n",
            "step 131 accuracy 0.9679\n",
            "step 132 accuracy 0.9679\n",
            "step 133 accuracy 0.9682\n",
            "step 134 accuracy 0.9682\n",
            "step 135 accuracy 0.9682\n",
            "step 136 accuracy 0.9682\n",
            "step 137 accuracy 0.9686\n",
            "step 138 accuracy 0.9686\n",
            "step 139 accuracy 0.9686\n",
            "step 140 accuracy 0.9686\n",
            "step 141 accuracy 0.9686\n",
            "step 142 accuracy 0.9686\n",
            "step 143 accuracy 0.9686\n",
            "step 144 accuracy 0.9686\n",
            "step 145 accuracy 0.9686\n",
            "step 146 accuracy 0.9686\n",
            "step 147 accuracy 0.9686\n",
            "step 148 accuracy 0.9686\n",
            "step 149 accuracy 0.9686\n",
            "step 150 accuracy 0.9686\n",
            "step 151 accuracy 0.9686\n",
            "step 152 accuracy 0.9686\n",
            "step 153 accuracy 0.9686\n",
            "step 154 accuracy 0.9686\n",
            "step 155 accuracy 0.9686\n",
            "step 156 accuracy 0.9686\n",
            "step 157 accuracy 0.9686\n",
            "step 158 accuracy 0.9686\n",
            "step 159 accuracy 0.9686\n",
            "step 160 accuracy 0.9686\n",
            "step 161 accuracy 0.9686\n",
            "step 162 accuracy 0.9686\n",
            "step 163 accuracy 0.9686\n",
            "step 164 accuracy 0.9686\n",
            "step 165 accuracy 0.9686\n",
            "step 166 accuracy 0.9686\n",
            "step 167 accuracy 0.9686\n",
            "step 168 accuracy 0.9686\n",
            "step 169 accuracy 0.9686\n",
            "step 170 accuracy 0.9686\n",
            "step 171 accuracy 0.9686\n",
            "step 172 accuracy 0.9686\n",
            "step 173 accuracy 0.9686\n",
            "step 174 accuracy 0.9690\n",
            "step 175 accuracy 0.9690\n",
            "step 176 accuracy 0.9690\n",
            "step 177 accuracy 0.9690\n",
            "step 178 accuracy 0.9690\n",
            "step 179 accuracy 0.9690\n",
            "step 180 accuracy 0.9690\n",
            "step 181 accuracy 0.9690\n",
            "step 182 accuracy 0.9690\n",
            "step 183 accuracy 0.9690\n",
            "step 184 accuracy 0.9690\n",
            "step 185 accuracy 0.9690\n",
            "step 186 accuracy 0.9690\n",
            "step 187 accuracy 0.9690\n",
            "step 188 accuracy 0.9690\n",
            "step 189 accuracy 0.9690\n",
            "step 190 accuracy 0.9690\n",
            "step 191 accuracy 0.9690\n",
            "step 192 accuracy 0.9690\n",
            "step 193 accuracy 0.9686\n",
            "step 194 accuracy 0.9686\n",
            "step 195 accuracy 0.9686\n",
            "step 196 accuracy 0.9686\n",
            "step 197 accuracy 0.9686\n",
            "step 198 accuracy 0.9686\n",
            "step 199 accuracy 0.9686\n",
            "step 200 accuracy 0.9686\n",
            "step 201 accuracy 0.9686\n",
            "step 202 accuracy 0.9686\n",
            "step 203 accuracy 0.9686\n",
            "step 204 accuracy 0.9686\n",
            "step 205 accuracy 0.9686\n",
            "step 206 accuracy 0.9690\n",
            "step 207 accuracy 0.9690\n",
            "step 208 accuracy 0.9694\n",
            "step 209 accuracy 0.9694\n",
            "step 210 accuracy 0.9694\n",
            "step 211 accuracy 0.9694\n",
            "step 212 accuracy 0.9694\n",
            "step 213 accuracy 0.9694\n",
            "step 214 accuracy 0.9694\n",
            "step 215 accuracy 0.9694\n",
            "step 216 accuracy 0.9694\n",
            "step 217 accuracy 0.9694\n",
            "step 218 accuracy 0.9694\n",
            "step 219 accuracy 0.9694\n",
            "step 220 accuracy 0.9694\n",
            "step 221 accuracy 0.9694\n",
            "step 222 accuracy 0.9694\n",
            "step 223 accuracy 0.9694\n",
            "step 224 accuracy 0.9694\n",
            "step 225 accuracy 0.9694\n",
            "step 226 accuracy 0.9694\n",
            "step 227 accuracy 0.9694\n",
            "step 228 accuracy 0.9694\n",
            "step 229 accuracy 0.9694\n",
            "step 230 accuracy 0.9694\n",
            "step 231 accuracy 0.9694\n",
            "step 232 accuracy 0.9694\n",
            "step 233 accuracy 0.9694\n",
            "step 234 accuracy 0.9694\n",
            "step 235 accuracy 0.9694\n",
            "step 236 accuracy 0.9694\n",
            "step 237 accuracy 0.9694\n",
            "step 238 accuracy 0.9694\n",
            "step 239 accuracy 0.9694\n",
            "step 240 accuracy 0.9694\n",
            "step 241 accuracy 0.9694\n",
            "step 242 accuracy 0.9694\n",
            "step 243 accuracy 0.9694\n",
            "step 244 accuracy 0.9694\n",
            "step 245 accuracy 0.9694\n",
            "step 246 accuracy 0.9694\n",
            "step 247 accuracy 0.9694\n",
            "step 248 accuracy 0.9694\n",
            "step 249 accuracy 0.9694\n",
            "step 250 accuracy 0.9694\n",
            "step 251 accuracy 0.9694\n",
            "step 252 accuracy 0.9694\n",
            "step 253 accuracy 0.9694\n",
            "step 254 accuracy 0.9694\n",
            "step 255 accuracy 0.9694\n",
            "step 256 accuracy 0.9694\n",
            "step 257 accuracy 0.9694\n",
            "step 258 accuracy 0.9694\n",
            "step 259 accuracy 0.9694\n",
            "step 260 accuracy 0.9694\n",
            "step 261 accuracy 0.9694\n",
            "step 262 accuracy 0.9694\n",
            "step 263 accuracy 0.9694\n",
            "step 264 accuracy 0.9694\n",
            "step 265 accuracy 0.9694\n",
            "step 266 accuracy 0.9694\n",
            "step 267 accuracy 0.9694\n",
            "step 268 accuracy 0.9694\n",
            "step 269 accuracy 0.9694\n",
            "step 270 accuracy 0.9694\n",
            "step 271 accuracy 0.9694\n",
            "step 272 accuracy 0.9694\n",
            "step 273 accuracy 0.9694\n",
            "step 274 accuracy 0.9694\n",
            "step 275 accuracy 0.9694\n",
            "step 276 accuracy 0.9694\n",
            "step 277 accuracy 0.9694\n",
            "step 278 accuracy 0.9694\n",
            "step 279 accuracy 0.9694\n",
            "step 280 accuracy 0.9694\n",
            "step 281 accuracy 0.9694\n",
            "step 282 accuracy 0.9694\n",
            "step 283 accuracy 0.9694\n",
            "step 284 accuracy 0.9694\n",
            "step 285 accuracy 0.9694\n",
            "step 286 accuracy 0.9694\n",
            "step 287 accuracy 0.9694\n",
            "step 288 accuracy 0.9694\n",
            "step 289 accuracy 0.9694\n",
            "step 290 accuracy 0.9694\n",
            "step 291 accuracy 0.9694\n",
            "step 292 accuracy 0.9694\n",
            "step 293 accuracy 0.9694\n",
            "step 294 accuracy 0.9694\n",
            "step 295 accuracy 0.9694\n",
            "step 296 accuracy 0.9694\n",
            "step 297 accuracy 0.9694\n",
            "step 298 accuracy 0.9694\n",
            "step 299 accuracy 0.9694\n",
            "step 300 accuracy 0.9694\n",
            "step 301 accuracy 0.9694\n",
            "step 302 accuracy 0.9694\n",
            "step 303 accuracy 0.9694\n",
            "step 304 accuracy 0.9694\n",
            "step 305 accuracy 0.9694\n",
            "step 306 accuracy 0.9694\n",
            "step 307 accuracy 0.9694\n",
            "step 308 accuracy 0.9694\n",
            "step 309 accuracy 0.9694\n",
            "step 310 accuracy 0.9694\n",
            "step 311 accuracy 0.9694\n",
            "step 312 accuracy 0.9694\n",
            "step 313 accuracy 0.9694\n",
            "step 314 accuracy 0.9694\n",
            "step 315 accuracy 0.9694\n",
            "step 316 accuracy 0.9694\n",
            "step 317 accuracy 0.9694\n",
            "step 318 accuracy 0.9694\n",
            "step 319 accuracy 0.9694\n",
            "step 320 accuracy 0.9694\n",
            "step 321 accuracy 0.9694\n",
            "step 322 accuracy 0.9694\n",
            "step 323 accuracy 0.9694\n",
            "step 324 accuracy 0.9694\n",
            "step 325 accuracy 0.9694\n",
            "step 326 accuracy 0.9694\n",
            "step 327 accuracy 0.9694\n",
            "step 328 accuracy 0.9694\n",
            "step 329 accuracy 0.9694\n",
            "step 330 accuracy 0.9694\n",
            "step 331 accuracy 0.9694\n",
            "step 332 accuracy 0.9694\n",
            "step 333 accuracy 0.9694\n",
            "step 334 accuracy 0.9694\n",
            "step 335 accuracy 0.9694\n",
            "step 336 accuracy 0.9694\n",
            "step 337 accuracy 0.9694\n",
            "step 338 accuracy 0.9694\n",
            "step 339 accuracy 0.9694\n",
            "step 340 accuracy 0.9694\n",
            "step 341 accuracy 0.9694\n",
            "step 342 accuracy 0.9694\n",
            "step 343 accuracy 0.9694\n",
            "step 344 accuracy 0.9694\n",
            "step 345 accuracy 0.9694\n",
            "step 346 accuracy 0.9694\n",
            "step 347 accuracy 0.9690\n",
            "step 348 accuracy 0.9690\n",
            "step 349 accuracy 0.9690\n",
            "step 350 accuracy 0.9690\n",
            "step 351 accuracy 0.9690\n",
            "step 352 accuracy 0.9690\n",
            "step 353 accuracy 0.9690\n",
            "step 354 accuracy 0.9690\n",
            "step 355 accuracy 0.9690\n",
            "step 356 accuracy 0.9690\n",
            "step 357 accuracy 0.9690\n",
            "step 358 accuracy 0.9690\n",
            "step 359 accuracy 0.9690\n",
            "step 360 accuracy 0.9690\n",
            "step 361 accuracy 0.9690\n",
            "step 362 accuracy 0.9690\n",
            "step 363 accuracy 0.9690\n",
            "step 364 accuracy 0.9690\n",
            "step 365 accuracy 0.9690\n",
            "step 366 accuracy 0.9690\n",
            "step 367 accuracy 0.9690\n",
            "step 368 accuracy 0.9690\n",
            "step 369 accuracy 0.9690\n",
            "step 370 accuracy 0.9690\n",
            "step 371 accuracy 0.9690\n",
            "step 372 accuracy 0.9690\n",
            "step 373 accuracy 0.9690\n",
            "step 374 accuracy 0.9690\n",
            "step 375 accuracy 0.9690\n",
            "step 376 accuracy 0.9690\n",
            "step 377 accuracy 0.9690\n",
            "step 378 accuracy 0.9690\n",
            "step 379 accuracy 0.9690\n",
            "step 380 accuracy 0.9690\n",
            "step 381 accuracy 0.9690\n",
            "step 382 accuracy 0.9690\n",
            "step 383 accuracy 0.9690\n",
            "step 384 accuracy 0.9690\n",
            "step 385 accuracy 0.9690\n",
            "step 386 accuracy 0.9690\n",
            "step 387 accuracy 0.9690\n",
            "step 388 accuracy 0.9690\n",
            "step 389 accuracy 0.9690\n",
            "step 390 accuracy 0.9690\n",
            "step 391 accuracy 0.9690\n",
            "step 392 accuracy 0.9690\n",
            "step 393 accuracy 0.9690\n",
            "step 394 accuracy 0.9690\n",
            "step 395 accuracy 0.9690\n",
            "step 396 accuracy 0.9690\n",
            "step 397 accuracy 0.9690\n",
            "step 398 accuracy 0.9690\n",
            "step 399 accuracy 0.9690\n",
            "step 400 accuracy 0.9690\n",
            "step 401 accuracy 0.9690\n",
            "step 402 accuracy 0.9690\n",
            "step 403 accuracy 0.9686\n",
            "step 404 accuracy 0.9686\n",
            "step 405 accuracy 0.9686\n",
            "step 406 accuracy 0.9686\n",
            "step 407 accuracy 0.9686\n",
            "step 408 accuracy 0.9686\n",
            "step 409 accuracy 0.9686\n",
            "step 410 accuracy 0.9686\n",
            "step 411 accuracy 0.9686\n",
            "step 412 accuracy 0.9686\n",
            "step 413 accuracy 0.9686\n",
            "step 414 accuracy 0.9686\n",
            "step 415 accuracy 0.9686\n",
            "step 416 accuracy 0.9686\n",
            "step 417 accuracy 0.9686\n",
            "step 418 accuracy 0.9686\n",
            "step 419 accuracy 0.9686\n",
            "step 420 accuracy 0.9686\n",
            "step 421 accuracy 0.9686\n",
            "step 422 accuracy 0.9686\n",
            "step 423 accuracy 0.9686\n",
            "step 424 accuracy 0.9686\n",
            "step 425 accuracy 0.9686\n",
            "step 426 accuracy 0.9686\n",
            "step 427 accuracy 0.9686\n",
            "step 428 accuracy 0.9686\n",
            "step 429 accuracy 0.9686\n",
            "step 430 accuracy 0.9686\n",
            "step 431 accuracy 0.9686\n",
            "step 432 accuracy 0.9686\n",
            "step 433 accuracy 0.9690\n",
            "step 434 accuracy 0.9690\n",
            "step 435 accuracy 0.9690\n",
            "step 436 accuracy 0.9690\n",
            "step 437 accuracy 0.9690\n",
            "step 438 accuracy 0.9690\n",
            "step 439 accuracy 0.9690\n",
            "step 440 accuracy 0.9690\n",
            "step 441 accuracy 0.9690\n",
            "step 442 accuracy 0.9690\n",
            "step 443 accuracy 0.9690\n",
            "step 444 accuracy 0.9690\n",
            "step 445 accuracy 0.9690\n",
            "step 446 accuracy 0.9690\n",
            "step 447 accuracy 0.9690\n",
            "step 448 accuracy 0.9690\n",
            "step 449 accuracy 0.9690\n",
            "step 450 accuracy 0.9690\n",
            "step 451 accuracy 0.9690\n",
            "step 452 accuracy 0.9690\n",
            "step 453 accuracy 0.9690\n",
            "step 454 accuracy 0.9690\n",
            "step 455 accuracy 0.9690\n",
            "step 456 accuracy 0.9690\n",
            "step 457 accuracy 0.9690\n",
            "step 458 accuracy 0.9690\n",
            "step 459 accuracy 0.9690\n",
            "step 460 accuracy 0.9690\n",
            "step 461 accuracy 0.9690\n",
            "step 462 accuracy 0.9690\n",
            "step 463 accuracy 0.9690\n",
            "step 464 accuracy 0.9690\n",
            "step 465 accuracy 0.9690\n",
            "step 466 accuracy 0.9690\n",
            "step 467 accuracy 0.9690\n",
            "step 468 accuracy 0.9690\n",
            "step 469 accuracy 0.9690\n",
            "step 470 accuracy 0.9690\n",
            "step 471 accuracy 0.9690\n",
            "step 472 accuracy 0.9690\n",
            "step 473 accuracy 0.9690\n",
            "step 474 accuracy 0.9690\n",
            "step 475 accuracy 0.9690\n",
            "step 476 accuracy 0.9690\n",
            "step 477 accuracy 0.9690\n",
            "step 478 accuracy 0.9690\n",
            "step 479 accuracy 0.9690\n",
            "step 480 accuracy 0.9690\n",
            "step 481 accuracy 0.9690\n",
            "step 482 accuracy 0.9690\n",
            "step 483 accuracy 0.9690\n",
            "step 484 accuracy 0.9690\n",
            "step 485 accuracy 0.9690\n",
            "step 486 accuracy 0.9690\n",
            "step 487 accuracy 0.9690\n",
            "step 488 accuracy 0.9690\n",
            "step 489 accuracy 0.9690\n",
            "step 490 accuracy 0.9690\n",
            "step 491 accuracy 0.9690\n",
            "step 492 accuracy 0.9690\n",
            "step 493 accuracy 0.9690\n",
            "step 494 accuracy 0.9690\n",
            "step 495 accuracy 0.9690\n",
            "step 496 accuracy 0.9690\n",
            "step 497 accuracy 0.9690\n",
            "step 498 accuracy 0.9690\n",
            "step 499 accuracy 0.9690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Harmonic Function ##########\n",
        "\n",
        "# Still under construction. Previously got the accuracy as %100 for Karate Dataset. Now trying to implement it for cora dataset.\n",
        "\n",
        "def cora_graph():\n",
        "  G = nx.Graph()\n",
        "  len(edges)\n",
        "  for i in range(len(edges)):\n",
        "    G.add_edge(edges[i][0],edges[i][1])\n",
        "  return G"
      ],
      "metadata": {
        "id": "T0bnxRCyFT20"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = cora_graph()"
      ],
      "metadata": {
        "id": "_Cytu-RlFXFD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predicted(G):\n",
        "  split = np.arange(2708)\n",
        "  np.random.shuffle(split)\n",
        "  for i in split[:100]:\n",
        "    G.nodes[i][\"label\"] = f'{classes[i][1]}'\n",
        "  predicted = nx.node_classification.harmonic_function(G, max_iter=30)\n",
        "  #print(predicted)\n",
        "  return predicted\n",
        "\n",
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  predicted(G)\n",
        "\n",
        "\n",
        "#print(len(predicted(G)))\n",
        "#print(len(classes))\n",
        "print(G.nodes)\n",
        "\n",
        "def resulted(G):\n",
        "\n",
        "  node_array = np.array(G.nodes)\n",
        "  prediction = np.array(predicted(G))\n",
        "  resulted = [None] * len(node_array)\n",
        "  resulted = np.array(resulted)\n",
        "\n",
        "  for i in range(len(node_array)):\n",
        "    resulted[node_array[i]] = prediction[i]\n",
        "\n",
        "  return resulted\n",
        "\n",
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  predicted(G)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w2idLyXFaLj",
        "outputId": "f2815261-6e6d-47a7-ea4a-f7fb7b1da2dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1254, 1852, 2399, 2, 172, 3, 411, 709, 4, 2144, 5, 6, 2134, 7, 8, 260, 752, 1529, 9, 175, 10, 11, 174, 455, 761, 1116, 1180, 1308, 1319, 1442, 1635, 1760, 1763, 1909, 1989, 2223, 2229, 2230, 2236, 2441, 2586, 12, 16, 13, 267, 924, 925, 927, 945, 1101, 1133, 1422, 1495, 1896, 2363, 2389, 2392, 2667, 14, 813, 1089, 2414, 15, 37, 201, 243, 300, 413, 1445, 1891, 1985, 2239, 2595, 17, 1939, 20, 27, 75, 125, 567, 857, 21, 2666, 22, 23, 1060, 24, 26, 173, 28, 863, 2498, 29, 303, 382, 539, 1489, 1490, 30, 235, 31, 285, 499, 510, 566, 841, 1413, 1868, 1982, 2039, 32, 33, 178, 34, 2331, 35, 957, 36, 960, 1592, 38, 90, 454, 1307, 1350, 1802, 2600, 42, 82, 189, 554, 581, 1258, 1606, 1607, 1608, 1717, 1839, 2396, 43, 1488, 1552, 44, 1255, 45, 251, 49, 64, 570, 50, 395, 52, 53, 1753, 54, 185, 415, 56, 290, 717, 819, 1194, 1313, 1588, 1602, 1808, 1883, 2142, 2182, 2599, 2642, 57, 878, 1208, 1286, 1323, 1427, 1836, 2141, 2698, 58, 186, 511, 851, 1518, 2057, 2058, 2059, 2064, 59, 60, 1124, 1599, 61, 467, 693, 2347, 62, 2385, 63, 1031, 65, 83, 1379, 66, 778, 1548, 67, 68, 69, 216, 1765, 313, 568, 650, 1251, 1431, 341, 2616, 357, 390, 376, 421, 312, 428, 651, 1056, 1073, 2327, 498, 409, 674, 699, 696, 493, 672, 828, 1306, 1332, 1691, 1914, 1986, 2073, 697, 507, 1318, 1589, 1840, 1855, 705, 1873, 2333, 2539, 753, 88, 128, 148, 149, 157, 187, 246, 293, 362, 368, 371, 404, 479, 940, 964, 977, 1021, 1243, 1252, 1372, 1508, 1516, 1519, 1522, 1530, 1532, 1671, 1741, 1768, 1993, 2079, 2166, 2170, 2174, 2183, 2282, 2482, 2505, 2558, 2635, 2696, 2608, 762, 2242, 763, 462, 764, 192, 883, 974, 884, 886, 1501, 885, 1262, 2025, 2513, 887, 193, 888, 233, 1437, 1641, 889, 509, 891, 890, 892, 1678, 893, 221, 1087, 1171, 1233, 1301, 1507, 1515, 1649, 1650, 2178, 2507, 894, 895, 896, 317, 1749, 2633, 2668, 898, 899, 1364, 900, 902, 901, 904, 199, 433, 906, 220, 327, 2701, 907, 1275, 908, 292, 909, 1061, 1525, 913, 758, 914, 223, 915, 881, 1798, 2604, 916, 484, 917, 926, 1085, 918, 919, 388, 1872, 2703, 920, 780, 921, 46, 158, 351, 377, 824, 928, 1106, 1112, 1339, 1540, 1541, 1542, 1591, 1628, 1825, 1885, 1922, 2655, 922, 1477, 923, 181, 646, 662, 1746, 1947, 1496, 2670, 930, 2069, 2159, 2523, 929, 932, 784, 843, 1110, 1326, 1773, 1784, 2659, 933, 565, 1135, 1137, 1139, 1774, 2050, 934, 2454, 935, 99, 1093, 1113, 1299, 1300, 2326, 936, 1050, 1081, 1921, 937, 647, 1404, 938, 71, 939, 638, 876, 1377, 1822, 2314, 2316, 2167, 2184, 941, 1155, 2404, 942, 1950, 943, 40, 1636, 1668, 944, 1813, 196, 1440, 946, 197, 947, 394, 453, 477, 478, 1234, 949, 51, 118, 124, 191, 560, 961, 1011, 1363, 1517, 1804, 1997, 1999, 2024, 2035, 2040, 2200, 2241, 2566, 2614, 951, 445, 767, 865, 952, 2337, 2405, 2538, 1457, 1834, 953, 619, 1152, 954, 200, 955, 956, 2352, 958, 2292, 959, 2272, 2294, 962, 77, 420, 549, 714, 1207, 1246, 1298, 1432, 1479, 1527, 1845, 1961, 1974, 1996, 2086, 2334, 2407, 2409, 2587, 2646, 963, 302, 1406, 1293, 966, 965, 967, 202, 968, 1748, 969, 203, 426, 970, 971, 2626, 1039, 978, 2622, 979, 1788, 980, 1091, 1324, 981, 982, 1799, 983, 984, 985, 364, 1126, 986, 1164, 987, 180, 414, 988, 123, 989, 990, 1158, 1327, 1703, 991, 766, 992, 993, 1861, 1904, 2108, 994, 2421, 995, 224, 998, 1046, 996, 399, 997, 793, 999, 1000, 518, 844, 1990, 1001, 206, 1002, 1899, 1003, 277, 283, 358, 1004, 299, 1142, 1851, 1005, 259, 1927, 2550, 1006, 1009, 1007, 328, 1008, 1365, 361, 1057, 1010, 1012, 685, 720, 1391, 2588, 1013, 1014, 1853, 1015, 208, 1016, 25, 72, 121, 151, 195, 234, 245, 291, 294, 301, 309, 334, 339, 356, 406, 418, 521, 555, 580, 649, 654, 655, 682, 684, 715, 718, 732, 733, 796, 868, 1044, 1111, 1128, 1160, 1203, 1220, 1224, 1354, 1378, 1392, 1468, 1651, 1667, 1669, 1779, 1838, 1908, 2015, 2203, 2312, 2315, 2416, 2457, 2461, 2524, 2528, 2568, 2575, 2580, 2627, 2643, 2657, 2658, 2660, 2661, 2663, 2664, 1017, 1018, 1019, 2567, 408, 1020, 1568, 1022, 1683, 1023, 1024, 1025, 183, 1035, 1026, 1366, 2319, 1027, 237, 1735, 1736, 1028, 209, 369, 817, 820, 1029, 1726, 2585, 1030, 288, 1380, 2487, 318, 1165, 1032, 226, 621, 622, 861, 2255, 2631, 1034, 1036, 107, 1037, 1596, 2572, 398, 1040, 1041, 482, 1295, 1042, 210, 1043, 1685, 211, 239, 1045, 1047, 393, 781, 830, 877, 1054, 1666, 1843, 2301, 2693, 2699, 1048, 84, 1049, 2261, 630, 2632, 1051, 430, 1052, 278, 1925, 1053, 1690, 1783, 2037, 2152, 2439, 2530, 1055, 1826, 229, 1058, 576, 1100, 1584, 2571, 1059, 270, 1062, 641, 1063, 85, 96, 1064, 1066, 1670, 2462, 386, 1065, 1067, 1068, 1340, 1634, 1069, 298, 1070, 273, 534, 540, 729, 1344, 2130, 1071, 2248, 1072, 271, 666, 1787, 1074, 86, 315, 396, 1398, 1075, 145, 777, 2010, 2090, 2150, 1076, 1566, 2317, 1077, 831, 2481, 2676, 1078, 1079, 1080, 213, 667, 2237, 249, 1458, 1082, 76, 146, 252, 349, 387, 410, 848, 1281, 1780, 1906, 1963, 1966, 1083, 214, 1084, 215, 597, 1086, 446, 2083, 1088, 1436, 2436, 322, 1090, 416, 1092, 2702, 1094, 2075, 2099, 1095, 217, 1096, 612, 1841, 2002, 2003, 2005, 2006, 2007, 2009, 2286, 1097, 279, 1098, 261, 698, 1099, 190, 308, 821, 1177, 1315, 1331, 1700, 1805, 2619, 1102, 1103, 434, 593, 595, 596, 688, 976, 1104, 1777, 2596, 1107, 1108, 948, 1109, 1755, 1772, 363, 1115, 1848, 1117, 495, 1118, 227, 373, 786, 1119, 1178, 1343, 329, 1120, 792, 1121, 1403, 1122, 1114, 1229, 1587, 2450, 1123, 487, 1621, 1970, 2324, 2447, 2383, 1125, 134, 603, 1226, 1227, 1620, 1127, 523, 2512, 853, 1129, 222, 1130, 2474, 716, 2477, 1131, 905, 1764, 1946, 1977, 1978, 1132, 1134, 1827, 1949, 1136, 136, 563, 1138, 114, 316, 481, 1140, 443, 1141, 1544, 1143, 1144, 1145, 228, 423, 1148, 442, 1149, 604, 614, 620, 625, 2260, 2264, 2273, 2293, 1150, 2594, 772, 807, 1153, 98, 633, 691, 701, 740, 814, 1157, 1769, 2044, 2307, 2411, 2420, 660, 1156, 1159, 2332, 1161, 333, 711, 1923, 2465, 1162, 230, 722, 747, 2554, 1163, 177, 1202, 2672, 788, 827, 1506, 1166, 1844, 1167, 1169, 1579, 2238, 1170, 87, 120, 436, 2380, 2615, 1172, 1173, 1174, 690, 2381, 1175, 1973, 1176, 2402, 2497, 1179, 1933, 1181, 1182, 1183, 176, 1184, 0, 1185, 1412, 1186, 1698, 1187, 238, 2412, 1188, 1189, 480, 1190, 798, 1819, 1191, 1192, 1193, 241, 1195, 401, 1745, 1196, 306, 1198, 1272, 1199, 642, 2321, 1200, 1201, 1419, 1204, 2597, 1205, 169, 2071, 815, 1209, 244, 1210, 359, 496, 1724, 1211, 1555, 1212, 1147, 1213, 2463, 1214, 2540, 1215, 198, 1216, 1217, 156, 1310, 1218, 1945, 1219, 109, 1221, 1222, 2546, 1223, 794, 803, 1225, 809, 1236, 1600, 1719, 2623, 873, 1967, 2448, 708, 1228, 452, 1230, 1231, 1232, 1994, 2446, 557, 1235, 707, 2449, 1237, 2105, 2109, 2110, 2601, 1239, 1238, 1240, 1242, 1241, 1244, 204, 1245, 836, 1590, 2395, 1247, 1486, 1248, 247, 304, 1249, 2158, 1250, 248, 2165, 1253, 1256, 1758, 1257, 525, 559, 879, 950, 1328, 1622, 2017, 459, 1623, 1259, 1418, 1260, 1595, 1261, 1263, 179, 255, 1264, 822, 834, 835, 1265, 1266, 231, 463, 1267, 370, 1455, 2564, 1268, 533, 1730, 1269, 799, 1304, 1593, 1597, 1750, 2467, 2561, 1270, 305, 522, 710, 1401, 2173, 2188, 2210, 2214, 2386, 2516, 2578, 2656, 1271, 1396, 1647, 2681, 461, 1273, 723, 1818, 1276, 282, 2611, 1277, 1278, 1874, 1279, 73, 205, 218, 219, 1638, 1912, 2428, 1280, 640, 2370, 1282, 2269, 1283, 2253, 2634, 1284, 1285, 2650, 207, 335, 343, 384, 500, 583, 584, 745, 771, 845, 846, 847, 1585, 1594, 1931, 2018, 2019, 2020, 1287, 324, 1849, 1288, 1289, 352, 866, 867, 1789, 2267, 2356, 1290, 338, 1291, 2054, 2707, 1292, 274, 578, 579, 2206, 2207, 2208, 2549, 2665, 365, 1294, 93, 170, 1393, 2432, 2480, 1296, 168, 254, 378, 1297, 1387, 2215, 367, 429, 1302, 1303, 1898, 1305, 1992, 1309, 497, 882, 1311, 337, 1312, 1559, 1565, 1314, 330, 256, 1316, 353, 501, 564, 1487, 1686, 1738, 2026, 2036, 2131, 2137, 1317, 562, 2148, 1321, 258, 1322, 1325, 440, 1673, 1274, 1329, 355, 1658, 1330, 2440, 263, 1334, 264, 1335, 789, 1337, 1356, 1338, 265, 266, 2022, 1341, 336, 1342, 506, 89, 512, 1345, 1407, 1869, 2101, 2132, 1346, 171, 513, 1751, 1752, 1767, 1347, 232, 1915, 1916, 2339, 1348, 456, 1349, 1629, 1711, 2244, 2369, 2649, 1351, 272, 435, 1352, 531, 538, 2106, 2688, 1353, 325, 1355, 1358, 1357, 1359, 1360, 184, 1528, 1361, 1362, 2486, 2485, 2605, 1367, 589, 671, 683, 694, 755, 1368, 1388, 1463, 2053, 2422, 2423, 2424, 2425, 2494, 2574, 645, 724, 1369, 130, 2041, 1370, 825, 1371, 468, 1151, 1373, 1374, 677, 1375, 276, 1376, 931, 2212, 1333, 1791, 1382, 1384, 1383, 1385, 2403, 880, 2051, 1386, 1390, 1484, 1485, 1800, 2570, 400, 1465, 1394, 1395, 1397, 1707, 1399, 669, 731, 1824, 2364, 2367, 1400, 250, 2639, 1863, 1924, 2322, 1402, 281, 1408, 47, 155, 307, 412, 492, 826, 2087, 2245, 2459, 1409, 91, 160, 810, 1410, 437, 1411, 1415, 286, 2080, 1416, 287, 427, 2092, 2123, 1417, 1420, 126, 154, 164, 451, 599, 618, 770, 860, 1498, 2235, 2323, 2355, 2637, 2682, 1421, 104, 1676, 1423, 289, 1424, 1425, 774, 790, 875, 1426, 1476, 2685, 1538, 1429, 1430, 1433, 1655, 1434, 1435, 296, 379, 850, 2628, 113, 439, 485, 2247, 1438, 1858, 1439, 1743, 1441, 1154, 1443, 1444, 1446, 1447, 1448, 1449, 1450, 1451, 748, 1452, 1453, 1454, 1456, 795, 842, 350, 1459, 1659, 1460, 1461, 19, 101, 331, 344, 765, 1688, 2085, 1462, 116, 163, 194, 1320, 319, 585, 1514, 1771, 2055, 2705, 1464, 392, 135, 275, 441, 1837, 2052, 2070, 2211, 1466, 2493, 1467, 2408, 1469, 405, 1471, 1675, 1470, 417, 447, 1889, 1979, 2081, 2706, 1473, 2453, 1474, 1475, 314, 808, 1693, 1414, 1478, 1381, 1481, 159, 1482, 1483, 1720, 2545, 1491, 520, 1492, 2089, 2343, 1493, 257, 1168, 1583, 1684, 1494, 1497, 102, 1500, 1511, 2393, 2394, 1499, 1502, 2199, 1503, 1504, 675, 1505, 1932, 310, 508, 1645, 2169, 1509, 1510, 546, 2205, 1512, 782, 1513, 161, 1586, 2021, 225, 550, 2176, 2204, 2673, 1520, 1701, 1521, 470, 903, 1523, 1694, 1524, 804, 805, 1680, 1526, 515, 320, 1533, 812, 2674, 1534, 1535, 801, 1536, 1716, 1537, 1539, 1632, 1543, 1545, 643, 590, 591, 1803, 1926, 2231, 2232, 2233, 2234, 2308, 1549, 1714, 1550, 1551, 2061, 1553, 1554, 1708, 465, 473, 1556, 1557, 2541, 517, 837, 1558, 1560, 1900, 1561, 2063, 1562, 1563, 1564, 1569, 438, 1570, 1786, 1571, 1575, 1706, 2377, 1572, 1105, 1576, 687, 1775, 1577, 797, 1578, 611, 1660, 2263, 1580, 1472, 1582, 2508, 150, 2431, 1598, 1601, 1897, 1603, 138, 1604, 1890, 2525, 2526, 48, 561, 721, 768, 773, 800, 1546, 2397, 1609, 431, 1610, 1611, 598, 1612, 588, 1792, 1793, 2217, 2218, 2219, 2224, 2225, 2683, 1614, 743, 751, 1615, 2359, 2360, 1616, 1617, 345, 1618, 1705, 1619, 975, 1937, 167, 1033, 1816, 2001, 1624, 854, 1625, 347, 1626, 1627, 403, 775, 2346, 2418, 348, 323, 475, 912, 1679, 1759, 1796, 2417, 2607, 1630, 94, 297, 661, 2686, 1631, 2185, 1633, 2573, 41, 95, 140, 162, 460, 592, 757, 1657, 1972, 2328, 2426, 2555, 2557, 2636, 39, 80, 153, 321, 519, 823, 1648, 2388, 2606, 1637, 1964, 466, 1940, 2695, 1639, 1640, 1642, 1762, 1643, 92, 284, 2584, 1644, 1646, 2419, 1652, 1653, 425, 681, 1613, 1654, 18, 1656, 1681, 1661, 1662, 402, 2531, 2532, 2534, 1663, 380, 1664, 832, 870, 1665, 2114, 2483, 1672, 166, 346, 1674, 1721, 2603, 2625, 97, 1677, 1682, 389, 486, 504, 530, 532, 542, 544, 656, 742, 785, 856, 1206, 1809, 1821, 1870, 1902, 1948, 1981, 2000, 2030, 2031, 2032, 2033, 2034, 2038, 2045, 2048, 2062, 2065, 2091, 2121, 2126, 2127, 2128, 2154, 2246, 2300, 2302, 2306, 2340, 2341, 2348, 2471, 2537, 2551, 2583, 2610, 2697, 2700, 1687, 1692, 2138, 1695, 374, 1696, 100, 1697, 1954, 340, 1782, 2145, 1699, 1810, 1702, 1704, 122, 759, 1756, 1757, 1919, 1920, 2350, 1709, 1710, 2522, 1689, 1881, 1712, 1713, 678, 776, 783, 806, 833, 1718, 385, 2484, 1733, 2084, 1722, 1785, 1723, 754, 1725, 2133, 1727, 1728, 1729, 391, 1731, 1732, 1734, 2368, 1737, 2147, 855, 2093, 2097, 2129, 2149, 2156, 1739, 1740, 397, 1742, 1480, 1744, 2256, 1747, 103, 240, 1754, 648, 1761, 526, 527, 528, 1878, 1965, 1766, 2351, 1934, 2390, 712, 1776, 1778, 871, 874, 1405, 1942, 2445, 1781, 2143, 212, 897, 1790, 105, 2609, 1794, 1795, 869, 1797, 816, 1801, 106, 2651, 1806, 1807, 1846, 1811, 1812, 366, 769, 1814, 1815, 1817, 1907, 1336, 1894, 1901, 1820, 129, 311, 381, 2186, 1823, 342, 1828, 137, 1829, 665, 2353, 1830, 2338, 1831, 610, 1832, 1833, 242, 1876, 670, 730, 802, 910, 911, 972, 1574, 2391, 2501, 1835, 728, 2191, 2504, 2290, 1842, 108, 1847, 1850, 444, 1854, 617, 1856, 594, 1857, 131, 663, 141, 1859, 74, 1860, 110, 1862, 111, 2694, 1864, 577, 2164, 2177, 2209, 1865, 448, 1866, 449, 1867, 787, 1871, 2078, 1875, 1877, 1879, 144, 1880, 1884, 1882, 829, 1886, 1887, 535, 1888, 818, 1892, 464, 1893, 1895, 458, 1903, 1905, 1910, 1911, 1913, 2536, 634, 864, 2049, 2304, 1928, 70, 268, 1930, 2618, 1929, 424, 547, 1935, 112, 1936, 139, 1941, 2478, 1943, 706, 1944, 469, 2043, 1951, 1952, 1953, 1918, 471, 1955, 1956, 472, 1957, 1958, 115, 1960, 1959, 262, 474, 1962, 476, 1968, 680, 2384, 1969, 1971, 1975, 839, 1976, 483, 1038, 2511, 2630, 1980, 739, 2096, 537, 2115, 2117, 1983, 2116, 2193, 1984, 514, 587, 1995, 1987, 548, 1988, 117, 1991, 360, 1998, 2004, 2299, 2008, 383, 2011, 488, 2014, 2012, 489, 2013, 490, 491, 2016, 2163, 494, 2023, 840, 2027, 558, 676, 791, 2028, 2413, 2029, 749, 2111, 2151, 2654, 652, 2042, 502, 503, 2046, 505, 2082, 2056, 852, 2060, 529, 536, 2112, 2113, 2066, 2067, 375, 2068, 2410, 516, 2072, 2074, 2076, 2602, 2077, 1197, 236, 524, 2094, 2095, 2107, 2100, 541, 2102, 2103, 2104, 849, 2118, 2120, 2122, 543, 2124, 545, 2125, 2135, 2136, 2687, 2139, 2140, 551, 552, 553, 2146, 556, 2155, 858, 2157, 2160, 2354, 2161, 2162, 569, 165, 2652, 1573, 2168, 2171, 571, 2172, 572, 2175, 450, 2179, 326, 2180, 573, 2181, 644, 2187, 2189, 55, 457, 574, 2190, 182, 188, 2192, 1567, 2194, 575, 859, 2195, 2196, 2197, 1605, 2198, 2201, 2202, 586, 2213, 2216, 422, 2220, 2221, 2222, 2226, 2227, 269, 2228, 147, 2565, 1531, 2240, 127, 2243, 2415, 600, 2357, 2358, 2249, 601, 2250, 602, 616, 2251, 2252, 2277, 2254, 605, 2279, 2257, 606, 623, 629, 2265, 2266, 2285, 2291, 2296, 2297, 2258, 607, 2259, 608, 609, 2262, 615, 624, 628, 2278, 2281, 2268, 2271, 2270, 2274, 2284, 2275, 2276, 626, 2280, 2283, 862, 2287, 2288, 2289, 2406, 2295, 2298, 627, 2303, 632, 2305, 2309, 637, 2310, 636, 2311, 2313, 635, 2318, 2320, 2325, 700, 78, 653, 2329, 2330, 657, 658, 2335, 659, 2342, 2344, 1715, 2345, 744, 132, 811, 2556, 2349, 354, 664, 668, 2361, 133, 2362, 582, 1917, 2690, 673, 2365, 2366, 2371, 2372, 2373, 2374, 2375, 2376, 2378, 2379, 679, 2382, 2387, 2398, 2400, 686, 2401, 119, 613, 2429, 1581, 2427, 2430, 419, 2433, 2434, 2435, 2437, 702, 2438, 703, 2098, 2442, 704, 2443, 2444, 872, 2451, 2452, 713, 372, 2455, 2456, 734, 2518, 2458, 2621, 2671, 2460, 2517, 2464, 2466, 142, 2468, 2469, 2470, 2472, 2473, 2475, 719, 2476, 2479, 760, 2488, 407, 2489, 143, 2490, 2491, 2492, 2495, 725, 2496, 737, 738, 2500, 2519, 2499, 2502, 2543, 280, 631, 2503, 726, 2506, 727, 735, 2509, 2510, 2514, 2515, 736, 2520, 2521, 2527, 741, 2529, 2684, 2533, 2336, 432, 639, 2542, 2544, 2547, 746, 2548, 2552, 2553, 2559, 2692, 2560, 750, 2562, 2563, 2569, 152, 689, 1938, 756, 2576, 2581, 2577, 2579, 2582, 2047, 2589, 1146, 2590, 2591, 2592, 2593, 695, 79, 295, 2598, 2088, 2617, 838, 1770, 2153, 2535, 2612, 2613, 332, 779, 1389, 2620, 692, 2624, 2662, 2629, 2638, 2640, 2641, 2644, 973, 2645, 2647, 2648, 253, 2653, 2669, 2675, 2677, 81, 1428, 2678, 2679, 2680, 2704, 2119, 2689, 2691, 1547]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.nodes(data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elrENqTghBMb",
        "outputId": "42fdd058-5b29-48c2-d38a-5dad6f1cc131"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NodeDataView({1: {'label': '1'}, 1254: {'label': '1'}, 1852: {'label': '1'}, 2399: {'label': '1'}, 2: {'label': '3'}, 172: {'label': '3'}, 3: {'label': '5'}, 411: {'label': '5'}, 709: {'label': '5'}, 4: {'label': '1'}, 2144: {'label': '1'}, 5: {'label': '3'}, 6: {'label': '3'}, 2134: {'label': '3'}, 7: {'label': '2'}, 8: {'label': '2'}, 260: {'label': '2'}, 752: {'label': '2'}, 1529: {'label': '2'}, 9: {'label': '0'}, 175: {'label': '0'}, 10: {'label': '2'}, 11: {'label': '2'}, 174: {'label': '3'}, 455: {'label': '2'}, 761: {'label': '4'}, 1116: {'label': '2'}, 1180: {'label': '3'}, 1308: {'label': '2'}, 1319: {'label': '1'}, 1442: {'label': '1'}, 1635: {'label': '2'}, 1760: {'label': '4'}, 1763: {'label': '3'}, 1909: {'label': '2'}, 1989: {'label': '2'}, 2223: {'label': '3'}, 2229: {'label': '3'}, 2230: {'label': '2'}, 2236: {'label': '2'}, 2441: {'label': '2'}, 2586: {'label': '2'}, 12: {'label': '2'}, 16: {'label': '2'}, 13: {'label': '2'}, 267: {'label': '2'}, 924: {'label': '2'}, 925: {'label': '2'}, 927: {'label': '2'}, 945: {'label': '2'}, 1101: {'label': '2'}, 1133: {'label': '2'}, 1422: {'label': '2'}, 1495: {'label': '2'}, 1896: {'label': '2'}, 2363: {'label': '2'}, 2389: {'label': '2'}, 2392: {'label': '2'}, 2667: {'label': '2'}, 14: {'label': '2'}, 813: {'label': '2'}, 1089: {'label': '0'}, 2414: {'label': '0'}, 15: {'label': '3'}, 37: {'label': '3'}, 201: {'label': '3'}, 243: {'label': '3'}, 300: {'label': '3'}, 413: {'label': '3'}, 1445: {'label': '3'}, 1891: {'label': '3'}, 1985: {'label': '3'}, 2239: {'label': '6'}, 2595: {'label': '2'}, 17: {'label': '5'}, 1939: {'label': '5'}, 20: {'label': '5'}, 27: {'label': '5'}, 75: {'label': '5'}, 125: {'label': '5'}, 567: {'label': '5'}, 857: {'label': '5'}, 21: {'label': '2'}, 2666: {'label': '2'}, 22: {'label': '2'}, 23: {'label': '1'}, 1060: {'label': '1'}, 24: {'label': '1'}, 26: {'label': '5'}, 173: {'label': '5'}, 28: {'label': '1'}, 863: {'label': '1'}, 2498: {'label': '1'}, 29: {'label': '1'}, 303: {'label': '2'}, 382: {'label': '1'}, 539: {'label': '5'}, 1489: {'label': '2'}, 1490: {'label': '1'}, 30: {'label': '6'}, 235: {'label': '5'}, 31: {'label': '5'}, 285: {'label': '5'}, 499: {'label': '5'}, 510: {'label': '5'}, 566: {'label': '5'}, 841: {'label': '5'}, 1413: {'label': '5'}, 1868: {'label': '5'}, 1982: {'label': '5'}, 2039: {'label': '5'}, 32: {'label': '3'}, 33: {'label': '3'}, 178: {'label': '3'}, 34: {'label': '3'}, 2331: {'label': '3'}, 35: {'label': '4'}, 957: {'label': '4'}, 36: {'label': '4'}, 960: {'label': '4'}, 1592: {'label': '3'}, 38: {'label': '3'}, 90: {'label': '3'}, 454: {'label': '3'}, 1307: {'label': '3'}, 1350: {'label': '3'}, 1802: {'label': '3'}, 2600: {'label': '0'}, 42: {'label': '1'}, 82: {'label': '1'}, 189: {'label': '2'}, 554: {'label': '1'}, 581: {'label': '1'}, 1258: {'label': '1'}, 1606: {'label': '1'}, 1607: {'label': '1'}, 1608: {'label': '1'}, 1717: {'label': '3'}, 1839: {'label': '1'}, 2396: {'label': '1'}, 43: {'label': '1'}, 1488: {'label': '2'}, 1552: {'label': '1'}, 44: {'label': '1'}, 1255: {'label': '1'}, 45: {'label': '4'}, 251: {'label': '6'}, 49: {'label': '4'}, 64: {'label': '4'}, 570: {'label': '4'}, 50: {'label': '2'}, 395: {'label': '2'}, 52: {'label': '2'}, 53: {'label': '2'}, 1753: {'label': '2'}, 54: {'label': '1'}, 185: {'label': '1'}, 415: {'label': '1'}, 56: {'label': '3'}, 290: {'label': '3'}, 717: {'label': '2'}, 819: {'label': '4'}, 1194: {'label': '1'}, 1313: {'label': '2'}, 1588: {'label': '2'}, 1602: {'label': '3'}, 1808: {'label': '3'}, 1883: {'label': '1'}, 2142: {'label': '0'}, 2182: {'label': '6'}, 2599: {'label': '2'}, 2642: {'label': '6'}, 57: {'label': '3'}, 878: {'label': '3'}, 1208: {'label': '2'}, 1286: {'label': '3'}, 1323: {'label': '0'}, 1427: {'label': '0'}, 1836: {'label': '6'}, 2141: {'label': '0'}, 2698: {'label': '3'}, 58: {'label': '5'}, 186: {'label': '5'}, 511: {'label': '5'}, 851: {'label': '5'}, 1518: {'label': '5'}, 2057: {'label': '5'}, 2058: {'label': '5'}, 2059: {'label': '5'}, 2064: {'label': '5'}, 59: {'label': '4'}, 60: {'label': '4'}, 1124: {'label': '4'}, 1599: {'label': '4'}, 61: {'label': '4'}, 467: {'label': '4'}, 693: {'label': '4'}, 2347: {'label': '4'}, 62: {'label': '4'}, 2385: {'label': '4'}, 63: {'label': '4'}, 1031: {'label': '4'}, 65: {'label': '1'}, 83: {'label': '1'}, 1379: {'label': '1'}, 66: {'label': '5'}, 778: {'label': '5'}, 1548: {'label': '5'}, 67: {'label': '3'}, 68: {'label': '3'}, 69: {'label': '3'}, 216: {'label': '4'}, 1765: {'label': '1'}, 313: {'label': '0'}, 568: {'label': '0'}, 650: {'label': '0'}, 1251: {'label': '0'}, 1431: {'label': '0'}, 341: {'label': '3'}, 2616: {'label': '3'}, 357: {'label': '4'}, 390: {'label': '4'}, 376: {'label': '2'}, 421: {'label': '3'}, 312: {'label': '3'}, 428: {'label': '3'}, 651: {'label': '3'}, 1056: {'label': '3'}, 1073: {'label': '3'}, 2327: {'label': '3'}, 498: {'label': '3'}, 409: {'label': '3'}, 674: {'label': '2'}, 699: {'label': '2'}, 696: {'label': '0'}, 493: {'label': '2'}, 672: {'label': '0'}, 828: {'label': '2'}, 1306: {'label': '3'}, 1332: {'label': '3'}, 1691: {'label': '3'}, 1914: {'label': '0'}, 1986: {'label': '2'}, 2073: {'label': '2'}, 697: {'label': '2'}, 507: {'label': '2'}, 1318: {'label': '2'}, 1589: {'label': '2'}, 1840: {'label': '2'}, 1855: {'label': '2'}, 705: {'label': '2'}, 1873: {'label': '2'}, 2333: {'label': '2'}, 2539: {'label': '2'}, 753: {'label': '6'}, 88: {'label': '6'}, 128: {'label': '6'}, 148: {'label': '6'}, 149: {'label': '6'}, 157: {'label': '6'}, 187: {'label': '6'}, 246: {'label': '6'}, 293: {'label': '6'}, 362: {'label': '6'}, 368: {'label': '6'}, 371: {'label': '1'}, 404: {'label': '6'}, 479: {'label': '6'}, 940: {'label': '6'}, 964: {'label': '6'}, 977: {'label': '6'}, 1021: {'label': '6'}, 1243: {'label': '4'}, 1252: {'label': '6'}, 1372: {'label': '6'}, 1508: {'label': '6'}, 1516: {'label': '6'}, 1519: {'label': '6'}, 1522: {'label': '6'}, 1530: {'label': '6'}, 1532: {'label': '6'}, 1671: {'label': '6'}, 1741: {'label': '6'}, 1768: {'label': '6'}, 1993: {'label': '6'}, 2079: {'label': '6'}, 2166: {'label': '6'}, 2170: {'label': '6'}, 2174: {'label': '6'}, 2183: {'label': '6'}, 2282: {'label': '6'}, 2482: {'label': '6'}, 2505: {'label': '6'}, 2558: {'label': '6'}, 2635: {'label': '6'}, 2696: {'label': '6'}, 2608: {'label': '4'}, 762: {'label': '6'}, 2242: {'label': '3'}, 763: {'label': '1'}, 462: {'label': '1'}, 764: {'label': '1'}, 192: {'label': '1'}, 883: {'label': '2'}, 974: {'label': '2'}, 884: {'label': '0'}, 886: {'label': '0'}, 1501: {'label': '0'}, 885: {'label': '0'}, 1262: {'label': '0'}, 2025: {'label': '0'}, 2513: {'label': '0'}, 887: {'label': '1'}, 193: {'label': '1'}, 888: {'label': '2'}, 233: {'label': '1'}, 1437: {'label': '1'}, 1641: {'label': '4'}, 889: {'label': '5'}, 509: {'label': '5'}, 891: {'label': '5'}, 890: {'label': '5'}, 892: {'label': '4'}, 1678: {'label': '6'}, 893: {'label': '6'}, 221: {'label': '4'}, 1087: {'label': '4'}, 1171: {'label': '6'}, 1233: {'label': '4'}, 1301: {'label': '6'}, 1507: {'label': '6'}, 1515: {'label': '6'}, 1649: {'label': '6'}, 1650: {'label': '6'}, 2178: {'label': '6'}, 2507: {'label': '6'}, 894: {'label': '5'}, 895: {'label': '5'}, 896: {'label': '3'}, 317: {'label': '3'}, 1749: {'label': '3'}, 2633: {'label': '3'}, 2668: {'label': '3'}, 898: {'label': '6'}, 899: {'label': '6'}, 1364: {'label': '6'}, 900: {'label': '1'}, 902: {'label': '1'}, 901: {'label': '1'}, 904: {'label': '6'}, 199: {'label': '5'}, 433: {'label': '6'}, 906: {'label': '1'}, 220: {'label': '2'}, 327: {'label': '1'}, 2701: {'label': '1'}, 907: {'label': '1'}, 1275: {'label': '5'}, 908: {'label': '5'}, 292: {'label': '1'}, 909: {'label': '1'}, 1061: {'label': '1'}, 1525: {'label': '1'}, 913: {'label': '1'}, 758: {'label': '1'}, 914: {'label': '1'}, 223: {'label': '1'}, 915: {'label': '0'}, 881: {'label': '0'}, 1798: {'label': '0'}, 2604: {'label': '0'}, 916: {'label': '2'}, 484: {'label': '2'}, 917: {'label': '2'}, 926: {'label': '2'}, 1085: {'label': '2'}, 918: {'label': '2'}, 919: {'label': '2'}, 388: {'label': '2'}, 1872: {'label': '2'}, 2703: {'label': '2'}, 920: {'label': '1'}, 780: {'label': '1'}, 921: {'label': '2'}, 46: {'label': '2'}, 158: {'label': '2'}, 351: {'label': '2'}, 377: {'label': '2'}, 824: {'label': '2'}, 928: {'label': '2'}, 1106: {'label': '2'}, 1112: {'label': '2'}, 1339: {'label': '2'}, 1540: {'label': '2'}, 1541: {'label': '2'}, 1542: {'label': '2'}, 1591: {'label': '2'}, 1628: {'label': '2'}, 1825: {'label': '2'}, 1885: {'label': '2'}, 1922: {'label': '2'}, 2655: {'label': '4'}, 922: {'label': '2'}, 1477: {'label': '2'}, 923: {'label': '2'}, 181: {'label': '2'}, 646: {'label': '2'}, 662: {'label': '2'}, 1746: {'label': '2'}, 1947: {'label': '2'}, 1496: {'label': '2'}, 2670: {'label': '2'}, 930: {'label': '2'}, 2069: {'label': '2'}, 2159: {'label': '2'}, 2523: {'label': '2'}, 929: {'label': '2'}, 932: {'label': '1'}, 784: {'label': '1'}, 843: {'label': '1'}, 1110: {'label': '1'}, 1326: {'label': '1'}, 1773: {'label': '1'}, 1784: {'label': '1'}, 2659: {'label': '1'}, 933: {'label': '1'}, 565: {'label': '5'}, 1135: {'label': '1'}, 1137: {'label': '1'}, 1139: {'label': '1'}, 1774: {'label': '1'}, 2050: {'label': '2'}, 934: {'label': '1'}, 2454: {'label': '1'}, 935: {'label': '2'}, 99: {'label': '2'}, 1093: {'label': '2'}, 1113: {'label': '2'}, 1299: {'label': '2'}, 1300: {'label': '2'}, 2326: {'label': '2'}, 936: {'label': '1'}, 1050: {'label': '1'}, 1081: {'label': '2'}, 1921: {'label': '1'}, 937: {'label': '1'}, 647: {'label': '1'}, 1404: {'label': '1'}, 938: {'label': '1'}, 71: {'label': '2'}, 939: {'label': '1'}, 638: {'label': '1'}, 876: {'label': '1'}, 1377: {'label': '1'}, 1822: {'label': '1'}, 2314: {'label': '1'}, 2316: {'label': '1'}, 2167: {'label': '6'}, 2184: {'label': '6'}, 941: {'label': '1'}, 1155: {'label': '1'}, 2404: {'label': '1'}, 942: {'label': '1'}, 1950: {'label': '2'}, 943: {'label': '1'}, 40: {'label': '2'}, 1636: {'label': '2'}, 1668: {'label': '1'}, 944: {'label': '6'}, 1813: {'label': '2'}, 196: {'label': '2'}, 1440: {'label': '2'}, 946: {'label': '1'}, 197: {'label': '1'}, 947: {'label': '4'}, 394: {'label': '4'}, 453: {'label': '5'}, 477: {'label': '4'}, 478: {'label': '4'}, 1234: {'label': '4'}, 949: {'label': '5'}, 51: {'label': '6'}, 118: {'label': '5'}, 124: {'label': '5'}, 191: {'label': '5'}, 560: {'label': '5'}, 961: {'label': '5'}, 1011: {'label': '6'}, 1363: {'label': '5'}, 1517: {'label': '6'}, 1804: {'label': '5'}, 1997: {'label': '5'}, 1999: {'label': '5'}, 2024: {'label': '5'}, 2035: {'label': '5'}, 2040: {'label': '5'}, 2200: {'label': '3'}, 2241: {'label': '5'}, 2566: {'label': '5'}, 2614: {'label': '5'}, 951: {'label': '1'}, 445: {'label': '1'}, 767: {'label': '1'}, 865: {'label': '4'}, 952: {'label': '1'}, 2337: {'label': '4'}, 2405: {'label': '1'}, 2538: {'label': '1'}, 1457: {'label': '1'}, 1834: {'label': '4'}, 953: {'label': '4'}, 619: {'label': '4'}, 1152: {'label': '4'}, 954: {'label': '3'}, 200: {'label': '3'}, 955: {'label': '3'}, 956: {'label': '4'}, 2352: {'label': '1'}, 958: {'label': '4'}, 2292: {'label': '4'}, 959: {'label': '4'}, 2272: {'label': '4'}, 2294: {'label': '4'}, 962: {'label': '2'}, 77: {'label': '3'}, 420: {'label': '1'}, 549: {'label': '2'}, 714: {'label': '1'}, 1207: {'label': '0'}, 1246: {'label': '2'}, 1298: {'label': '1'}, 1432: {'label': '5'}, 1479: {'label': '0'}, 1527: {'label': '3'}, 1845: {'label': '5'}, 1961: {'label': '0'}, 1974: {'label': '0'}, 1996: {'label': '1'}, 2086: {'label': '0'}, 2334: {'label': '2'}, 2407: {'label': '0'}, 2409: {'label': '0'}, 2587: {'label': '0'}, 2646: {'label': '0'}, 963: {'label': '1'}, 302: {'label': '1'}, 1406: {'label': '1'}, 1293: {'label': '6'}, 966: {'label': '2'}, 965: {'label': '2'}, 967: {'label': '3'}, 202: {'label': '3'}, 968: {'label': '5'}, 1748: {'label': '5'}, 969: {'label': '3'}, 203: {'label': '3'}, 426: {'label': '3'}, 970: {'label': '3'}, 971: {'label': '1'}, 2626: {'label': '1'}, 1039: {'label': '6'}, 978: {'label': '0'}, 2622: {'label': '0'}, 979: {'label': '1'}, 1788: {'label': '4'}, 980: {'label': '1'}, 1091: {'label': '1'}, 1324: {'label': '1'}, 981: {'label': '1'}, 982: {'label': '1'}, 1799: {'label': '1'}, 983: {'label': '1'}, 984: {'label': '1'}, 985: {'label': '1'}, 364: {'label': '1'}, 1126: {'label': '1'}, 986: {'label': '6'}, 1164: {'label': '6'}, 987: {'label': '6'}, 180: {'label': '6'}, 414: {'label': '6'}, 988: {'label': '6'}, 123: {'label': '6'}, 989: {'label': '6'}, 990: {'label': '1'}, 1158: {'label': '6'}, 1327: {'label': '6'}, 1703: {'label': '2'}, 991: {'label': '6'}, 766: {'label': '2'}, 992: {'label': '4'}, 993: {'label': '5'}, 1861: {'label': '5'}, 1904: {'label': '5'}, 2108: {'label': '5'}, 994: {'label': '2'}, 2421: {'label': '2'}, 995: {'label': '1'}, 224: {'label': '1'}, 998: {'label': '1'}, 1046: {'label': '1'}, 996: {'label': '1'}, 399: {'label': '1'}, 997: {'label': '1'}, 793: {'label': '1'}, 999: {'label': '1'}, 1000: {'label': '0'}, 518: {'label': '0'}, 844: {'label': '0'}, 1990: {'label': '0'}, 1001: {'label': '2'}, 206: {'label': '2'}, 1002: {'label': '2'}, 1899: {'label': '6'}, 1003: {'label': '6'}, 277: {'label': '6'}, 283: {'label': '6'}, 358: {'label': '6'}, 1004: {'label': '5'}, 299: {'label': '5'}, 1142: {'label': '5'}, 1851: {'label': '5'}, 1005: {'label': '5'}, 259: {'label': '1'}, 1927: {'label': '5'}, 2550: {'label': '3'}, 1006: {'label': '4'}, 1009: {'label': '4'}, 1007: {'label': '4'}, 328: {'label': '4'}, 1008: {'label': '4'}, 1365: {'label': '4'}, 361: {'label': '4'}, 1057: {'label': '4'}, 1010: {'label': '4'}, 1012: {'label': '1'}, 685: {'label': '1'}, 720: {'label': '1'}, 1391: {'label': '1'}, 2588: {'label': '1'}, 1013: {'label': '0'}, 1014: {'label': '0'}, 1853: {'label': '0'}, 1015: {'label': '3'}, 208: {'label': '3'}, 1016: {'label': '1'}, 25: {'label': '1'}, 72: {'label': '1'}, 121: {'label': '6'}, 151: {'label': '1'}, 195: {'label': '1'}, 234: {'label': '1'}, 245: {'label': '1'}, 291: {'label': '1'}, 294: {'label': '1'}, 301: {'label': '1'}, 309: {'label': '1'}, 334: {'label': '6'}, 339: {'label': '1'}, 356: {'label': '1'}, 406: {'label': '1'}, 418: {'label': '1'}, 521: {'label': '1'}, 555: {'label': '1'}, 580: {'label': '6'}, 649: {'label': '1'}, 654: {'label': '1'}, 655: {'label': '1'}, 682: {'label': '1'}, 684: {'label': '1'}, 715: {'label': '1'}, 718: {'label': '1'}, 732: {'label': '1'}, 733: {'label': '1'}, 796: {'label': '1'}, 868: {'label': '1'}, 1044: {'label': '1'}, 1111: {'label': '1'}, 1128: {'label': '1'}, 1160: {'label': '1'}, 1203: {'label': '1'}, 1220: {'label': '1'}, 1224: {'label': '6'}, 1354: {'label': '1'}, 1378: {'label': '1'}, 1392: {'label': '1'}, 1468: {'label': '1'}, 1651: {'label': '1'}, 1667: {'label': '1'}, 1669: {'label': '1'}, 1779: {'label': '1'}, 1838: {'label': '1'}, 1908: {'label': '1'}, 2015: {'label': '1'}, 2203: {'label': '6'}, 2312: {'label': '1'}, 2315: {'label': '1'}, 2416: {'label': '1'}, 2457: {'label': '1'}, 2461: {'label': '1'}, 2524: {'label': '1'}, 2528: {'label': '1'}, 2568: {'label': '1'}, 2575: {'label': '1'}, 2580: {'label': '1'}, 2627: {'label': '1'}, 2643: {'label': '1'}, 2657: {'label': '1'}, 2658: {'label': '1'}, 2660: {'label': '1'}, 2661: {'label': '1'}, 2663: {'label': '1'}, 2664: {'label': '1'}, 1017: {'label': '1'}, 1018: {'label': '1'}, 1019: {'label': '1'}, 2567: {'label': '1'}, 408: {'label': '1'}, 1020: {'label': '1'}, 1568: {'label': '1'}, 1022: {'label': '0'}, 1683: {'label': '2'}, 1023: {'label': '2'}, 1024: {'label': '0'}, 1025: {'label': '1'}, 183: {'label': '6'}, 1035: {'label': '1'}, 1026: {'label': '4'}, 1366: {'label': '4'}, 2319: {'label': '4'}, 1027: {'label': '0'}, 237: {'label': '3'}, 1735: {'label': '0'}, 1736: {'label': '0'}, 1028: {'label': '0'}, 209: {'label': '0'}, 369: {'label': '0'}, 817: {'label': '0'}, 820: {'label': '0'}, 1029: {'label': '0'}, 1726: {'label': '2'}, 2585: {'label': '1'}, 1030: {'label': '2'}, 288: {'label': '2'}, 1380: {'label': '2'}, 2487: {'label': '2'}, 318: {'label': '4'}, 1165: {'label': '4'}, 1032: {'label': '4'}, 226: {'label': '4'}, 621: {'label': '4'}, 622: {'label': '4'}, 861: {'label': '4'}, 2255: {'label': '4'}, 2631: {'label': '4'}, 1034: {'label': '0'}, 1036: {'label': '1'}, 107: {'label': '1'}, 1037: {'label': '6'}, 1596: {'label': '1'}, 2572: {'label': '2'}, 398: {'label': '6'}, 1040: {'label': '6'}, 1041: {'label': '6'}, 482: {'label': '6'}, 1295: {'label': '6'}, 1042: {'label': '1'}, 210: {'label': '1'}, 1043: {'label': '2'}, 1685: {'label': '2'}, 211: {'label': '1'}, 239: {'label': '1'}, 1045: {'label': '2'}, 1047: {'label': '5'}, 393: {'label': '5'}, 781: {'label': '5'}, 830: {'label': '5'}, 877: {'label': '5'}, 1054: {'label': '5'}, 1666: {'label': '5'}, 1843: {'label': '5'}, 2301: {'label': '5'}, 2693: {'label': '5'}, 2699: {'label': '5'}, 1048: {'label': '1'}, 84: {'label': '1'}, 1049: {'label': '4'}, 2261: {'label': '4'}, 630: {'label': '1'}, 2632: {'label': '1'}, 1051: {'label': '1'}, 430: {'label': '1'}, 1052: {'label': '5'}, 278: {'label': '5'}, 1925: {'label': '5'}, 1053: {'label': '5'}, 1690: {'label': '5'}, 1783: {'label': '5'}, 2037: {'label': '5'}, 2152: {'label': '5'}, 2439: {'label': '5'}, 2530: {'label': '5'}, 1055: {'label': '1'}, 1826: {'label': '3'}, 229: {'label': '4'}, 1058: {'label': '3'}, 576: {'label': '3'}, 1100: {'label': '3'}, 1584: {'label': '3'}, 2571: {'label': '5'}, 1059: {'label': '4'}, 270: {'label': '4'}, 1062: {'label': '5'}, 641: {'label': '5'}, 1063: {'label': '4'}, 85: {'label': '4'}, 96: {'label': '4'}, 1064: {'label': '4'}, 1066: {'label': '4'}, 1670: {'label': '4'}, 2462: {'label': '4'}, 386: {'label': '4'}, 1065: {'label': '4'}, 1067: {'label': '2'}, 1068: {'label': '1'}, 1340: {'label': '1'}, 1634: {'label': '1'}, 1069: {'label': '3'}, 298: {'label': '5'}, 1070: {'label': '5'}, 273: {'label': '5'}, 534: {'label': '5'}, 540: {'label': '5'}, 729: {'label': '5'}, 1344: {'label': '5'}, 2130: {'label': '5'}, 1071: {'label': '1'}, 2248: {'label': '1'}, 1072: {'label': '1'}, 271: {'label': '1'}, 666: {'label': '1'}, 1787: {'label': '2'}, 1074: {'label': '3'}, 86: {'label': '3'}, 315: {'label': '3'}, 396: {'label': '3'}, 1398: {'label': '3'}, 1075: {'label': '3'}, 145: {'label': '0'}, 777: {'label': '3'}, 2010: {'label': '3'}, 2090: {'label': '0'}, 2150: {'label': '3'}, 1076: {'label': '0'}, 1566: {'label': '0'}, 2317: {'label': '0'}, 1077: {'label': '6'}, 831: {'label': '6'}, 2481: {'label': '6'}, 2676: {'label': '6'}, 1078: {'label': '1'}, 1079: {'label': '2'}, 1080: {'label': '1'}, 213: {'label': '1'}, 667: {'label': '1'}, 2237: {'label': '2'}, 249: {'label': '1'}, 1458: {'label': '2'}, 1082: {'label': '3'}, 76: {'label': '3'}, 146: {'label': '3'}, 252: {'label': '3'}, 349: {'label': '3'}, 387: {'label': '0'}, 410: {'label': '3'}, 848: {'label': '3'}, 1281: {'label': '3'}, 1780: {'label': '3'}, 1906: {'label': '3'}, 1963: {'label': '3'}, 1966: {'label': '3'}, 1083: {'label': '5'}, 214: {'label': '5'}, 1084: {'label': '5'}, 215: {'label': '5'}, 597: {'label': '5'}, 1086: {'label': '2'}, 446: {'label': '2'}, 2083: {'label': '4'}, 1088: {'label': '1'}, 1436: {'label': '1'}, 2436: {'label': '1'}, 322: {'label': '0'}, 1090: {'label': '1'}, 416: {'label': '1'}, 1092: {'label': '2'}, 2702: {'label': '2'}, 1094: {'label': '5'}, 2075: {'label': '5'}, 2099: {'label': '5'}, 1095: {'label': '1'}, 217: {'label': '1'}, 1096: {'label': '4'}, 612: {'label': '4'}, 1841: {'label': '4'}, 2002: {'label': '4'}, 2003: {'label': '4'}, 2005: {'label': '4'}, 2006: {'label': '4'}, 2007: {'label': '4'}, 2009: {'label': '4'}, 2286: {'label': '4'}, 1097: {'label': '6'}, 279: {'label': '6'}, 1098: {'label': '1'}, 261: {'label': '1'}, 698: {'label': '1'}, 1099: {'label': '1'}, 190: {'label': '2'}, 308: {'label': '2'}, 821: {'label': '2'}, 1177: {'label': '2'}, 1315: {'label': '2'}, 1331: {'label': '2'}, 1700: {'label': '2'}, 1805: {'label': '2'}, 2619: {'label': '2'}, 1102: {'label': '2'}, 1103: {'label': '4'}, 434: {'label': '2'}, 593: {'label': '1'}, 595: {'label': '1'}, 596: {'label': '1'}, 688: {'label': '2'}, 976: {'label': '4'}, 1104: {'label': '0'}, 1777: {'label': '0'}, 2596: {'label': '0'}, 1107: {'label': '2'}, 1108: {'label': '5'}, 948: {'label': '5'}, 1109: {'label': '4'}, 1755: {'label': '1'}, 1772: {'label': '1'}, 363: {'label': '1'}, 1115: {'label': '4'}, 1848: {'label': '4'}, 1117: {'label': '1'}, 495: {'label': '1'}, 1118: {'label': '3'}, 227: {'label': '3'}, 373: {'label': '3'}, 786: {'label': '3'}, 1119: {'label': '3'}, 1178: {'label': '3'}, 1343: {'label': '3'}, 329: {'label': '3'}, 1120: {'label': '1'}, 792: {'label': '1'}, 1121: {'label': '1'}, 1403: {'label': '1'}, 1122: {'label': '4'}, 1114: {'label': '4'}, 1229: {'label': '4'}, 1587: {'label': '4'}, 2450: {'label': '4'}, 1123: {'label': '4'}, 487: {'label': '4'}, 1621: {'label': '4'}, 1970: {'label': '4'}, 2324: {'label': '4'}, 2447: {'label': '4'}, 2383: {'label': '4'}, 1125: {'label': '4'}, 134: {'label': '4'}, 603: {'label': '4'}, 1226: {'label': '4'}, 1227: {'label': '4'}, 1620: {'label': '4'}, 1127: {'label': '0'}, 523: {'label': '0'}, 2512: {'label': '0'}, 853: {'label': '1'}, 1129: {'label': '1'}, 222: {'label': '1'}, 1130: {'label': '1'}, 2474: {'label': '1'}, 716: {'label': '1'}, 2477: {'label': '1'}, 1131: {'label': '4'}, 905: {'label': '4'}, 1764: {'label': '4'}, 1946: {'label': '4'}, 1977: {'label': '4'}, 1978: {'label': '4'}, 1132: {'label': '1'}, 1134: {'label': '1'}, 1827: {'label': '1'}, 1949: {'label': '1'}, 1136: {'label': '1'}, 136: {'label': '1'}, 563: {'label': '1'}, 1138: {'label': '1'}, 114: {'label': '1'}, 316: {'label': '1'}, 481: {'label': '1'}, 1140: {'label': '1'}, 443: {'label': '1'}, 1141: {'label': '1'}, 1544: {'label': '5'}, 1143: {'label': '2'}, 1144: {'label': '3'}, 1145: {'label': '2'}, 228: {'label': '2'}, 423: {'label': '2'}, 1148: {'label': '4'}, 442: {'label': '4'}, 1149: {'label': '4'}, 604: {'label': '4'}, 614: {'label': '4'}, 620: {'label': '4'}, 625: {'label': '4'}, 2260: {'label': '4'}, 2264: {'label': '4'}, 2273: {'label': '4'}, 2293: {'label': '4'}, 1150: {'label': '6'}, 2594: {'label': '6'}, 772: {'label': '4'}, 807: {'label': '4'}, 1153: {'label': '5'}, 98: {'label': '5'}, 633: {'label': '5'}, 691: {'label': '5'}, 701: {'label': '5'}, 740: {'label': '5'}, 814: {'label': '5'}, 1157: {'label': '5'}, 1769: {'label': '5'}, 2044: {'label': '5'}, 2307: {'label': '5'}, 2411: {'label': '5'}, 2420: {'label': '5'}, 660: {'label': '1'}, 1156: {'label': '1'}, 1159: {'label': '3'}, 2332: {'label': '3'}, 1161: {'label': '4'}, 333: {'label': '4'}, 711: {'label': '4'}, 1923: {'label': '2'}, 2465: {'label': '1'}, 1162: {'label': '2'}, 230: {'label': '2'}, 722: {'label': '1'}, 747: {'label': '1'}, 2554: {'label': '2'}, 1163: {'label': '6'}, 177: {'label': '2'}, 1202: {'label': '6'}, 2672: {'label': '6'}, 788: {'label': '4'}, 827: {'label': '4'}, 1506: {'label': '4'}, 1166: {'label': '1'}, 1844: {'label': '1'}, 1167: {'label': '4'}, 1169: {'label': '2'}, 1579: {'label': '1'}, 2238: {'label': '1'}, 1170: {'label': '2'}, 87: {'label': '6'}, 120: {'label': '6'}, 436: {'label': '6'}, 2380: {'label': '6'}, 2615: {'label': '6'}, 1172: {'label': '5'}, 1173: {'label': '6'}, 1174: {'label': '5'}, 690: {'label': '5'}, 2381: {'label': '5'}, 1175: {'label': '1'}, 1973: {'label': '2'}, 1176: {'label': '1'}, 2402: {'label': '1'}, 2497: {'label': '2'}, 1179: {'label': '3'}, 1933: {'label': '3'}, 1181: {'label': '5'}, 1182: {'label': '5'}, 1183: {'label': '5'}, 176: {'label': '5'}, 1184: {'label': '0'}, 0: {'label': '0'}, 1185: {'label': '6'}, 1412: {'label': '6'}, 1186: {'label': '0'}, 1698: {'label': '1'}, 1187: {'label': '5'}, 238: {'label': '5'}, 2412: {'label': '5'}, 1188: {'label': '5'}, 1189: {'label': '1'}, 480: {'label': '1'}, 1190: {'label': '1'}, 798: {'label': '1'}, 1819: {'label': '1'}, 1191: {'label': '1'}, 1192: {'label': '0'}, 1193: {'label': '4'}, 241: {'label': '4'}, 1195: {'label': '1'}, 401: {'label': '1'}, 1745: {'label': '1'}, 1196: {'label': '5'}, 306: {'label': '5'}, 1198: {'label': '6'}, 1272: {'label': '6'}, 1199: {'label': '5'}, 642: {'label': '5'}, 2321: {'label': '5'}, 1200: {'label': '3'}, 1201: {'label': '6'}, 1419: {'label': '1'}, 1204: {'label': '1'}, 2597: {'label': '1'}, 1205: {'label': '1'}, 169: {'label': '1'}, 2071: {'label': '2'}, 815: {'label': '0'}, 1209: {'label': '3'}, 244: {'label': '3'}, 1210: {'label': '0'}, 359: {'label': '0'}, 496: {'label': '0'}, 1724: {'label': '0'}, 1211: {'label': '4'}, 1555: {'label': '4'}, 1212: {'label': '4'}, 1147: {'label': '2'}, 1213: {'label': '4'}, 2463: {'label': '4'}, 1214: {'label': '4'}, 2540: {'label': '4'}, 1215: {'label': '2'}, 198: {'label': '1'}, 1216: {'label': '4'}, 1217: {'label': '4'}, 156: {'label': '2'}, 1310: {'label': '4'}, 1218: {'label': '4'}, 1945: {'label': '4'}, 1219: {'label': '3'}, 109: {'label': '1'}, 1221: {'label': '6'}, 1222: {'label': '6'}, 2546: {'label': '6'}, 1223: {'label': '6'}, 794: {'label': '6'}, 803: {'label': '6'}, 1225: {'label': '4'}, 809: {'label': '4'}, 1236: {'label': '4'}, 1600: {'label': '4'}, 1719: {'label': '4'}, 2623: {'label': '4'}, 873: {'label': '4'}, 1967: {'label': '4'}, 2448: {'label': '4'}, 708: {'label': '4'}, 1228: {'label': '4'}, 452: {'label': '4'}, 1230: {'label': '4'}, 1231: {'label': '4'}, 1232: {'label': '4'}, 1994: {'label': '4'}, 2446: {'label': '4'}, 557: {'label': '4'}, 1235: {'label': '4'}, 707: {'label': '4'}, 2449: {'label': '4'}, 1237: {'label': '5'}, 2105: {'label': '5'}, 2109: {'label': '5'}, 2110: {'label': '5'}, 2601: {'label': '5'}, 1239: {'label': '1'}, 1238: {'label': '5'}, 1240: {'label': '5'}, 1242: {'label': '1'}, 1241: {'label': '1'}, 1244: {'label': '4'}, 204: {'label': '4'}, 1245: {'label': '2'}, 836: {'label': '2'}, 1590: {'label': '2'}, 2395: {'label': '2'}, 1247: {'label': '2'}, 1486: {'label': '2'}, 1248: {'label': '1'}, 247: {'label': '1'}, 304: {'label': '1'}, 1249: {'label': '0'}, 2158: {'label': '0'}, 1250: {'label': '0'}, 248: {'label': '6'}, 2165: {'label': '6'}, 1253: {'label': '1'}, 1256: {'label': '4'}, 1758: {'label': '4'}, 1257: {'label': '1'}, 525: {'label': '1'}, 559: {'label': '1'}, 879: {'label': '1'}, 950: {'label': '1'}, 1328: {'label': '1'}, 1622: {'label': '1'}, 2017: {'label': '1'}, 459: {'label': '1'}, 1623: {'label': '1'}, 1259: {'label': '6'}, 1418: {'label': '6'}, 1260: {'label': '3'}, 1595: {'label': '3'}, 1261: {'label': '3'}, 1263: {'label': '3'}, 179: {'label': '3'}, 255: {'label': '3'}, 1264: {'label': '3'}, 822: {'label': '3'}, 834: {'label': '3'}, 835: {'label': '3'}, 1265: {'label': '6'}, 1266: {'label': '3'}, 231: {'label': '3'}, 463: {'label': '2'}, 1267: {'label': '3'}, 370: {'label': '0'}, 1455: {'label': '0'}, 2564: {'label': '3'}, 1268: {'label': '5'}, 533: {'label': '5'}, 1730: {'label': '5'}, 1269: {'label': '3'}, 799: {'label': '3'}, 1304: {'label': '3'}, 1593: {'label': '3'}, 1597: {'label': '3'}, 1750: {'label': '3'}, 2467: {'label': '3'}, 2561: {'label': '3'}, 1270: {'label': '1'}, 305: {'label': '5'}, 522: {'label': '1'}, 710: {'label': '1'}, 1401: {'label': '5'}, 2173: {'label': '6'}, 2188: {'label': '6'}, 2210: {'label': '1'}, 2214: {'label': '1'}, 2386: {'label': '1'}, 2516: {'label': '1'}, 2578: {'label': '1'}, 2656: {'label': '1'}, 1271: {'label': '1'}, 1396: {'label': '2'}, 1647: {'label': '4'}, 2681: {'label': '2'}, 461: {'label': '6'}, 1273: {'label': '1'}, 723: {'label': '1'}, 1818: {'label': '0'}, 1276: {'label': '5'}, 282: {'label': '3'}, 2611: {'label': '5'}, 1277: {'label': '3'}, 1278: {'label': '1'}, 1874: {'label': '1'}, 1279: {'label': '4'}, 73: {'label': '2'}, 205: {'label': '2'}, 218: {'label': '1'}, 219: {'label': '4'}, 1638: {'label': '4'}, 1912: {'label': '2'}, 2428: {'label': '4'}, 1280: {'label': '2'}, 640: {'label': '3'}, 2370: {'label': '2'}, 1282: {'label': '1'}, 2269: {'label': '1'}, 1283: {'label': '3'}, 2253: {'label': '3'}, 2634: {'label': '3'}, 1284: {'label': '3'}, 1285: {'label': '3'}, 2650: {'label': '3'}, 207: {'label': '3'}, 335: {'label': '3'}, 343: {'label': '3'}, 384: {'label': '3'}, 500: {'label': '3'}, 583: {'label': '3'}, 584: {'label': '3'}, 745: {'label': '3'}, 771: {'label': '3'}, 845: {'label': '3'}, 846: {'label': '3'}, 847: {'label': '3'}, 1585: {'label': '3'}, 1594: {'label': '1'}, 1931: {'label': '3'}, 2018: {'label': '3'}, 2019: {'label': '3'}, 2020: {'label': '3'}, 1287: {'label': '4'}, 324: {'label': '4'}, 1849: {'label': '4'}, 1288: {'label': '4'}, 1289: {'label': '1'}, 352: {'label': '1'}, 866: {'label': '1'}, 867: {'label': '1'}, 1789: {'label': '1'}, 2267: {'label': '4'}, 2356: {'label': '1'}, 1290: {'label': '1'}, 338: {'label': '1'}, 1291: {'label': '1'}, 2054: {'label': '1'}, 2707: {'label': '1'}, 1292: {'label': '1'}, 274: {'label': '1'}, 578: {'label': '1'}, 579: {'label': '1'}, 2206: {'label': '1'}, 2207: {'label': '1'}, 2208: {'label': '1'}, 2549: {'label': '1'}, 2665: {'label': '1'}, 365: {'label': '6'}, 1294: {'label': '3'}, 93: {'label': '1'}, 170: {'label': '6'}, 1393: {'label': '6'}, 2432: {'label': '3'}, 2480: {'label': '6'}, 1296: {'label': '1'}, 168: {'label': '1'}, 254: {'label': '2'}, 378: {'label': '2'}, 1297: {'label': '1'}, 1387: {'label': '2'}, 2215: {'label': '2'}, 367: {'label': '2'}, 429: {'label': '2'}, 1302: {'label': '5'}, 1303: {'label': '0'}, 1898: {'label': '0'}, 1305: {'label': '0'}, 1992: {'label': '3'}, 1309: {'label': '0'}, 497: {'label': '0'}, 882: {'label': '0'}, 1311: {'label': '4'}, 337: {'label': '4'}, 1312: {'label': '4'}, 1559: {'label': '4'}, 1565: {'label': '4'}, 1314: {'label': '6'}, 330: {'label': '2'}, 256: {'label': '4'}, 1316: {'label': '5'}, 353: {'label': '5'}, 501: {'label': '5'}, 564: {'label': '5'}, 1487: {'label': '5'}, 1686: {'label': '5'}, 1738: {'label': '5'}, 2026: {'label': '5'}, 2036: {'label': '5'}, 2131: {'label': '5'}, 2137: {'label': '5'}, 1317: {'label': '5'}, 562: {'label': '5'}, 2148: {'label': '5'}, 1321: {'label': '4'}, 258: {'label': '4'}, 1322: {'label': '4'}, 1325: {'label': '1'}, 440: {'label': '1'}, 1673: {'label': '1'}, 1274: {'label': '1'}, 1329: {'label': '0'}, 355: {'label': '2'}, 1658: {'label': '2'}, 1330: {'label': '2'}, 2440: {'label': '2'}, 263: {'label': '2'}, 1334: {'label': '1'}, 264: {'label': '1'}, 1335: {'label': '6'}, 789: {'label': '6'}, 1337: {'label': '3'}, 1356: {'label': '3'}, 1338: {'label': '4'}, 265: {'label': '4'}, 266: {'label': '1'}, 2022: {'label': '1'}, 1341: {'label': '3'}, 336: {'label': '3'}, 1342: {'label': '5'}, 506: {'label': '5'}, 89: {'label': '5'}, 512: {'label': '5'}, 1345: {'label': '5'}, 1407: {'label': '5'}, 1869: {'label': '5'}, 2101: {'label': '5'}, 2132: {'label': '5'}, 1346: {'label': '3'}, 171: {'label': '3'}, 513: {'label': '3'}, 1751: {'label': '3'}, 1752: {'label': '3'}, 1767: {'label': '3'}, 1347: {'label': '0'}, 232: {'label': '0'}, 1915: {'label': '0'}, 1916: {'label': '0'}, 2339: {'label': '0'}, 1348: {'label': '1'}, 456: {'label': '2'}, 1349: {'label': '3'}, 1629: {'label': '1'}, 1711: {'label': '2'}, 2244: {'label': '1'}, 2369: {'label': '3'}, 2649: {'label': '1'}, 1351: {'label': '1'}, 272: {'label': '1'}, 435: {'label': '1'}, 1352: {'label': '5'}, 531: {'label': '5'}, 538: {'label': '5'}, 2106: {'label': '5'}, 2688: {'label': '5'}, 1353: {'label': '1'}, 325: {'label': '1'}, 1355: {'label': '3'}, 1358: {'label': '3'}, 1357: {'label': '3'}, 1359: {'label': '2'}, 1360: {'label': '3'}, 184: {'label': '3'}, 1528: {'label': '3'}, 1361: {'label': '1'}, 1362: {'label': '1'}, 2486: {'label': '4'}, 2485: {'label': '4'}, 2605: {'label': '4'}, 1367: {'label': '1'}, 589: {'label': '1'}, 671: {'label': '1'}, 683: {'label': '1'}, 694: {'label': '1'}, 755: {'label': '1'}, 1368: {'label': '1'}, 1388: {'label': '1'}, 1463: {'label': '1'}, 2053: {'label': '1'}, 2422: {'label': '1'}, 2423: {'label': '1'}, 2424: {'label': '1'}, 2425: {'label': '1'}, 2494: {'label': '1'}, 2574: {'label': '1'}, 645: {'label': '4'}, 724: {'label': '1'}, 1369: {'label': '2'}, 130: {'label': '4'}, 2041: {'label': '5'}, 1370: {'label': '1'}, 825: {'label': '1'}, 1371: {'label': '5'}, 468: {'label': '5'}, 1151: {'label': '6'}, 1373: {'label': '0'}, 1374: {'label': '0'}, 677: {'label': '0'}, 1375: {'label': '1'}, 276: {'label': '1'}, 1376: {'label': '1'}, 931: {'label': '1'}, 2212: {'label': '1'}, 1333: {'label': '0'}, 1791: {'label': '2'}, 1382: {'label': '0'}, 1384: {'label': '0'}, 1383: {'label': '0'}, 1385: {'label': '0'}, 2403: {'label': '0'}, 880: {'label': '0'}, 2051: {'label': '0'}, 1386: {'label': '0'}, 1390: {'label': '2'}, 1484: {'label': '2'}, 1485: {'label': '2'}, 1800: {'label': '0'}, 2570: {'label': '4'}, 400: {'label': '1'}, 1465: {'label': '1'}, 1394: {'label': '0'}, 1395: {'label': '2'}, 1397: {'label': '3'}, 1707: {'label': '3'}, 1399: {'label': '1'}, 669: {'label': '1'}, 731: {'label': '1'}, 1824: {'label': '1'}, 2364: {'label': '1'}, 2367: {'label': '1'}, 1400: {'label': '1'}, 250: {'label': '1'}, 2639: {'label': '1'}, 1863: {'label': '5'}, 1924: {'label': '5'}, 2322: {'label': '5'}, 1402: {'label': '2'}, 281: {'label': '1'}, 1408: {'label': '0'}, 47: {'label': '0'}, 155: {'label': '0'}, 307: {'label': '0'}, 412: {'label': '0'}, 492: {'label': '0'}, 826: {'label': '0'}, 2087: {'label': '0'}, 2245: {'label': '0'}, 2459: {'label': '0'}, 1409: {'label': '4'}, 91: {'label': '4'}, 160: {'label': '4'}, 810: {'label': '4'}, 1410: {'label': '6'}, 437: {'label': '1'}, 1411: {'label': '6'}, 1415: {'label': '1'}, 286: {'label': '1'}, 2080: {'label': '1'}, 1416: {'label': '5'}, 287: {'label': '5'}, 427: {'label': '5'}, 2092: {'label': '5'}, 2123: {'label': '5'}, 1417: {'label': '5'}, 1420: {'label': '2'}, 126: {'label': '1'}, 154: {'label': '1'}, 164: {'label': '1'}, 451: {'label': '1'}, 599: {'label': '1'}, 618: {'label': '4'}, 770: {'label': '1'}, 860: {'label': '1'}, 1498: {'label': '2'}, 2235: {'label': '1'}, 2323: {'label': '1'}, 2355: {'label': '4'}, 2637: {'label': '1'}, 2682: {'label': '4'}, 1421: {'label': '1'}, 104: {'label': '1'}, 1676: {'label': '1'}, 1423: {'label': '3'}, 289: {'label': '1'}, 1424: {'label': '5'}, 1425: {'label': '1'}, 774: {'label': '1'}, 790: {'label': '1'}, 875: {'label': '1'}, 1426: {'label': '1'}, 1476: {'label': '1'}, 2685: {'label': '1'}, 1538: {'label': '1'}, 1429: {'label': '4'}, 1430: {'label': '1'}, 1433: {'label': '3'}, 1655: {'label': '3'}, 1434: {'label': '1'}, 1435: {'label': '1'}, 296: {'label': '1'}, 379: {'label': '1'}, 850: {}, 2628: {'label': '1'}, 113: {'label': '4'}, 439: {'label': '2'}, 485: {'label': '3'}, 2247: {'label': '1'}, 1438: {'label': '1'}, 1858: {'label': '1'}, 1439: {'label': '1'}, 1743: {'label': '2'}, 1441: {'label': '1'}, 1154: {'label': '1'}, 1443: {'label': '5'}, 1444: {'label': '5'}, 1446: {'label': '1'}, 1447: {'label': '2'}, 1448: {'label': '3'}, 1449: {'label': '1'}, 1450: {'label': '5'}, 1451: {'label': '5'}, 748: {'label': '5'}, 1452: {'label': '5'}, 1453: {'label': '5'}, 1454: {'label': '0'}, 1456: {'label': '5'}, 795: {'label': '5'}, 842: {'label': '5'}, 350: {'label': '1'}, 1459: {'label': '1'}, 1659: {'label': '1'}, 1460: {'label': '5'}, 1461: {'label': '2'}, 19: {'label': '2'}, 101: {'label': '2'}, 331: {'label': '2'}, 344: {'label': '2'}, 765: {'label': '2'}, 1688: {'label': '2'}, 2085: {'label': '2'}, 1462: {'label': '2'}, 116: {'label': '2'}, 163: {'label': '1'}, 194: {'label': '2'}, 1320: {'label': '2'}, 319: {'label': '1'}, 585: {'label': '1'}, 1514: {'label': '1'}, 1771: {'label': '4'}, 2055: {'label': '1'}, 2705: {'label': '1'}, 1464: {'label': '1'}, 392: {'label': '1'}, 135: {'label': '1'}, 275: {'label': '1'}, 441: {'label': '1'}, 1837: {'label': '1'}, 2052: {'label': '1'}, 2070: {'label': '1'}, 2211: {'label': '1'}, 1466: {'label': '1'}, 2493: {'label': '3'}, 1467: {'label': '1'}, 2408: {'label': '1'}, 1469: {'label': '1'}, 405: {'label': '1'}, 1471: {'label': '1'}, 1675: {'label': '1'}, 1470: {'label': '1'}, 417: {'label': '1'}, 447: {'label': '1'}, 1889: {'label': '1'}, 1979: {'label': '1'}, 2081: {'label': '1'}, 2706: {'label': '1'}, 1473: {'label': '1'}, 2453: {'label': '1'}, 1474: {'label': '1'}, 1475: {'label': '1'}, 314: {'label': '2'}, 808: {'label': '1'}, 1693: {'label': '1'}, 1414: {'label': '2'}, 1478: {'label': '2'}, 1381: {'label': '2'}, 1481: {'label': '2'}, 159: {'label': '0'}, 1482: {'label': '2'}, 1483: {'label': '1'}, 1720: {'label': '2'}, 2545: {'label': '2'}, 1491: {'label': '1'}, 520: {'label': '1'}, 1492: {'label': '1'}, 2089: {'label': '0'}, 2343: {'label': '2'}, 1493: {'label': '4'}, 257: {'label': '1'}, 1168: {'label': '4'}, 1583: {'label': '0'}, 1684: {'label': '1'}, 1494: {'label': '1'}, 1497: {'label': '1'}, 102: {'label': '6'}, 1500: {'label': '6'}, 1511: {'label': '1'}, 2393: {'label': '1'}, 2394: {'label': '1'}, 1499: {'label': '3'}, 1502: {'label': '1'}, 2199: {'label': '1'}, 1503: {'label': '1'}, 1504: {'label': '2'}, 675: {'label': '3'}, 1505: {'label': '3'}, 1932: {'label': '3'}, 310: {'label': '6'}, 508: {'label': '6'}, 1645: {'label': '6'}, 2169: {'label': '6'}, 1509: {'label': '4'}, 1510: {'label': '6'}, 546: {'label': '6'}, 2205: {'label': '6'}, 1512: {'label': '3'}, 782: {'label': '3'}, 1513: {'label': '3'}, 161: {'label': '6'}, 1586: {'label': '6'}, 2021: {'label': '1'}, 225: {'label': '6'}, 550: {'label': '6'}, 2176: {'label': '6'}, 2204: {'label': '6'}, 2673: {'label': '6'}, 1520: {'label': '6'}, 1701: {'label': '6'}, 1521: {'label': '6'}, 470: {'label': '6'}, 903: {'label': '6'}, 1523: {'label': '6'}, 1694: {'label': '1'}, 1524: {'label': '1'}, 804: {'label': '1'}, 805: {'label': '1'}, 1680: {'label': '1'}, 1526: {'label': '6'}, 515: {'label': '3'}, 320: {'label': '1'}, 1533: {'label': '6'}, 812: {'label': '6'}, 2674: {'label': '6'}, 1534: {'label': '6'}, 1535: {'label': '6'}, 801: {'label': '6'}, 1536: {'label': '6'}, 1716: {'label': '6'}, 1537: {'label': '6'}, 1539: {'label': '1'}, 1632: {'label': '2'}, 1543: {'label': '2'}, 1545: {'label': '5'}, 643: {'label': '5'}, 590: {'label': '5'}, 591: {'label': '5'}, 1803: {'label': '5'}, 1926: {'label': '5'}, 2231: {'label': '5'}, 2232: {'label': '5'}, 2233: {'label': '5'}, 2234: {'label': '5'}, 2308: {'label': '5'}, 1549: {'label': '6'}, 1714: {'label': '6'}, 1550: {'label': '2'}, 1551: {'label': '5'}, 2061: {'label': '5'}, 1553: {'label': '1'}, 1554: {'label': '4'}, 1708: {'label': '4'}, 465: {'label': '4'}, 473: {'label': '3'}, 1556: {'label': '5'}, 1557: {'label': '5'}, 2541: {'label': '5'}, 517: {'label': '5'}, 837: {'label': '5'}, 1558: {'label': '5'}, 1560: {'label': '1'}, 1900: {'label': '1'}, 1561: {'label': '5'}, 2063: {'label': '5'}, 1562: {'label': '6'}, 1563: {'label': '2'}, 1564: {'label': '4'}, 1569: {'label': '1'}, 438: {'label': '1'}, 1570: {'label': '1'}, 1786: {'label': '1'}, 1571: {'label': '4'}, 1575: {'label': '4'}, 1706: {'label': '1'}, 2377: {'label': '4'}, 1572: {'label': '4'}, 1105: {'label': '4'}, 1576: {'label': '1'}, 687: {'label': '4'}, 1775: {'label': '1'}, 1577: {'label': '4'}, 797: {'label': '4'}, 1578: {'label': '5'}, 611: {'label': '5'}, 1660: {'label': '5'}, 2263: {'label': '5'}, 1580: {'label': '5'}, 1472: {'label': '5'}, 1582: {'label': '1'}, 2508: {'label': '1'}, 150: {'label': '3'}, 2431: {'label': '3'}, 1598: {'label': '1'}, 1601: {'label': '2'}, 1897: {'label': '2'}, 1603: {'label': '1'}, 138: {'label': '1'}, 1604: {'label': '3'}, 1890: {'label': '3'}, 2525: {'label': '3'}, 2526: {'label': '3'}, 48: {'label': '1'}, 561: {'label': '1'}, 721: {'label': '2'}, 768: {'label': '1'}, 773: {'label': '1'}, 800: {'label': '1'}, 1546: {'label': '1'}, 2397: {'label': '1'}, 1609: {'label': '1'}, 431: {'label': '1'}, 1610: {'label': '4'}, 1611: {'label': '4'}, 598: {'label': '4'}, 1612: {'label': '4'}, 588: {'label': '4'}, 1792: {'label': '4'}, 1793: {'label': '4'}, 2217: {'label': '4'}, 2218: {'label': '4'}, 2219: {'label': '4'}, 2224: {'label': '4'}, 2225: {'label': '4'}, 2683: {'label': '4'}, 1614: {'label': '5'}, 743: {'label': '3'}, 751: {'label': '3'}, 1615: {'label': '1'}, 2359: {'label': '1'}, 2360: {'label': '1'}, 1616: {'label': '4'}, 1617: {'label': '0'}, 345: {'label': '0'}, 1618: {'label': '4'}, 1705: {'label': '2'}, 1619: {'label': '4'}, 975: {'label': '4'}, 1937: {'label': '4'}, 167: {'label': '4'}, 1033: {'label': '4'}, 1816: {'label': '4'}, 2001: {'label': '4'}, 1624: {'label': '1'}, 854: {'label': '1'}, 1625: {'label': '5'}, 347: {'label': '5'}, 1626: {'label': '2'}, 1627: {'label': '0'}, 403: {'label': '2'}, 775: {'label': '2'}, 2346: {'label': '0'}, 2418: {'label': '0'}, 348: {'label': '6'}, 323: {'label': '1'}, 475: {'label': '3'}, 912: {'label': '3'}, 1679: {'label': '2'}, 1759: {'label': '3'}, 1796: {'label': '3'}, 2417: {'label': '2'}, 2607: {'label': '3'}, 1630: {'label': '3'}, 94: {'label': '2'}, 297: {'label': '4'}, 661: {'label': '4'}, 2686: {'label': '4'}, 1631: {'label': '2'}, 2185: {'label': '2'}, 1633: {'label': '2'}, 2573: {'label': '1'}, 41: {'label': '5'}, 95: {'label': '5'}, 140: {'label': '1'}, 162: {'label': '1'}, 460: {'label': '5'}, 592: {'label': '1'}, 757: {'label': '1'}, 1657: {'label': '1'}, 1972: {'label': '4'}, 2328: {'label': '5'}, 2426: {'label': '1'}, 2555: {'label': '5'}, 2557: {'label': '5'}, 2636: {'label': '1'}, 39: {'label': '2'}, 80: {'label': '3'}, 153: {'label': '3'}, 321: {'label': '0'}, 519: {'label': '2'}, 823: {'label': '0'}, 1648: {'label': '1'}, 2388: {'label': '0'}, 2606: {'label': '3'}, 1637: {'label': '2'}, 1964: {'label': '3'}, 466: {'label': '4'}, 1940: {'label': '2'}, 2695: {'label': '2'}, 1639: {'label': '4'}, 1640: {'label': '2'}, 1642: {'label': '2'}, 1762: {'label': '3'}, 1643: {'label': '2'}, 92: {'label': '2'}, 284: {'label': '6'}, 2584: {'label': '1'}, 1644: {'label': '1'}, 1646: {'label': '1'}, 2419: {'label': '1'}, 1652: {'label': '1'}, 1653: {'label': '4'}, 425: {'label': '4'}, 681: {'label': '4'}, 1613: {'label': '4'}, 1654: {'label': '3'}, 18: {'label': '3'}, 1656: {'label': '2'}, 1681: {'label': '1'}, 1661: {'label': '3'}, 1662: {'label': '1'}, 402: {'label': '1'}, 2531: {'label': '1'}, 2532: {'label': '1'}, 2534: {'label': '1'}, 1663: {'label': '1'}, 380: {'label': '1'}, 1664: {'label': '5'}, 832: {'label': '5'}, 870: {'label': '5'}, 1665: {'label': '5'}, 2114: {'label': '5'}, 2483: {'label': '1'}, 1672: {'label': '1'}, 166: {'label': '1'}, 346: {'label': '1'}, 1674: {'label': '1'}, 1721: {'label': '1'}, 2603: {'label': '1'}, 2625: {'label': '1'}, 97: {'label': '1'}, 1677: {'label': '1'}, 1682: {'label': '0'}, 389: {'label': '5'}, 486: {'label': '5'}, 504: {'label': '5'}, 530: {'label': '5'}, 532: {'label': '5'}, 542: {'label': '5'}, 544: {'label': '5'}, 656: {'label': '5'}, 742: {'label': '5'}, 785: {'label': '5'}, 856: {'label': '5'}, 1206: {'label': '5'}, 1809: {'label': '5'}, 1821: {'label': '5'}, 1870: {'label': '5'}, 1902: {'label': '5'}, 1948: {'label': '5'}, 1981: {'label': '5'}, 2000: {'label': '5'}, 2030: {'label': '5'}, 2031: {'label': '5'}, 2032: {'label': '5'}, 2033: {'label': '5'}, 2034: {'label': '5'}, 2038: {'label': '5'}, 2045: {'label': '5'}, 2048: {'label': '5'}, 2062: {'label': '5'}, 2065: {'label': '5'}, 2091: {'label': '5'}, 2121: {'label': '5'}, 2126: {'label': '5'}, 2127: {'label': '5'}, 2128: {'label': '5'}, 2154: {'label': '5'}, 2246: {'label': '5'}, 2300: {'label': '5'}, 2302: {'label': '5'}, 2306: {'label': '5'}, 2340: {'label': '5'}, 2341: {'label': '5'}, 2348: {'label': '5'}, 2471: {'label': '5'}, 2537: {'label': '5'}, 2551: {'label': '3'}, 2583: {'label': '5'}, 2610: {'label': '5'}, 2697: {'label': '5'}, 2700: {'label': '5'}, 1687: {'label': '1'}, 1692: {'label': '1'}, 2138: {'label': '1'}, 1695: {'label': '4'}, 374: {'label': '4'}, 1696: {'label': '3'}, 100: {'label': '3'}, 1697: {'label': '4'}, 1954: {'label': '4'}, 340: {'label': '1'}, 1782: {'label': '1'}, 2145: {'label': '1'}, 1699: {'label': '6'}, 1810: {'label': '6'}, 1702: {'label': '4'}, 1704: {'label': '4'}, 122: {'label': '4'}, 759: {'label': '1'}, 1756: {'label': '4'}, 1757: {'label': '4'}, 1919: {'label': '4'}, 1920: {'label': '4'}, 2350: {'label': '1'}, 1709: {'label': '2'}, 1710: {'label': '4'}, 2522: {'label': '4'}, 1689: {'label': '2'}, 1881: {'label': '2'}, 1712: {'label': '1'}, 1713: {'label': '4'}, 678: {'label': '1'}, 776: {'label': '3'}, 783: {'label': '3'}, 806: {'label': '3'}, 833: {'label': '3'}, 1718: {'label': '1'}, 385: {'label': '1'}, 2484: {'label': '1'}, 1733: {'label': '4'}, 2084: {'label': '4'}, 1722: {'label': '1'}, 1785: {'label': '1'}, 1723: {'label': '1'}, 754: {'label': '1'}, 1725: {'label': '3'}, 2133: {'label': '3'}, 1727: {'label': '5'}, 1728: {'label': '4'}, 1729: {'label': '3'}, 391: {'label': '3'}, 1731: {'label': '4'}, 1732: {'label': '4'}, 1734: {'label': '3'}, 2368: {'label': '3'}, 1737: {'label': '1'}, 2147: {'label': '1'}, 855: {'label': '5'}, 2093: {'label': '5'}, 2097: {'label': '5'}, 2129: {'label': '5'}, 2149: {'label': '5'}, 2156: {'label': '5'}, 1739: {'label': '3'}, 1740: {'label': '1'}, 397: {'label': '1'}, 1742: {'label': '3'}, 1480: {'label': '3'}, 1744: {'label': '4'}, 2256: {'label': '4'}, 1747: {'label': '1'}, 103: {'label': '1'}, 240: {'label': '3'}, 1754: {'label': '1'}, 648: {'label': '1'}, 1761: {'label': '0'}, 526: {'label': '0'}, 527: {'label': '0'}, 528: {'label': '0'}, 1878: {'label': '0'}, 1965: {'label': '2'}, 1766: {'label': '1'}, 2351: {'label': '1'}, 1934: {'label': '3'}, 2390: {'label': '4'}, 712: {'label': '1'}, 1776: {'label': '5'}, 1778: {'label': '4'}, 871: {'label': '4'}, 874: {'label': '4'}, 1405: {'label': '4'}, 1942: {'label': '4'}, 2445: {'label': '4'}, 1781: {'label': '6'}, 2143: {'label': '1'}, 212: {'label': '5'}, 897: {'label': '6'}, 1790: {'label': '1'}, 105: {'label': '2'}, 2609: {'label': '4'}, 1794: {'label': '1'}, 1795: {'label': '4'}, 869: {'label': '4'}, 1797: {'label': '1'}, 816: {'label': '1'}, 1801: {'label': '1'}, 106: {'label': '4'}, 2651: {'label': '2'}, 1806: {'label': '2'}, 1807: {'label': '1'}, 1846: {'label': '5'}, 1811: {'label': '4'}, 1812: {'label': '1'}, 366: {'label': '1'}, 769: {'label': '1'}, 1814: {'label': '2'}, 1815: {'label': '2'}, 1817: {'label': '5'}, 1907: {'label': '5'}, 1336: {'label': '3'}, 1894: {'label': '3'}, 1901: {'label': '1'}, 1820: {'label': '3'}, 129: {'label': '0'}, 311: {'label': '3'}, 381: {'label': '2'}, 2186: {'label': '5'}, 1823: {'label': '5'}, 342: {'label': '1'}, 1828: {'label': '1'}, 137: {'label': '1'}, 1829: {'label': '1'}, 665: {'label': '1'}, 2353: {'label': '1'}, 1830: {'label': '4'}, 2338: {'label': '1'}, 1831: {'label': '1'}, 610: {'label': '1'}, 1832: {'label': '1'}, 1833: {'label': '4'}, 242: {'label': '1'}, 1876: {'label': '4'}, 670: {'label': '1'}, 730: {'label': '1'}, 802: {'label': '4'}, 910: {'label': '1'}, 911: {'label': '1'}, 972: {'label': '4'}, 1574: {'label': '1'}, 2391: {'label': '1'}, 2501: {'label': '1'}, 1835: {'label': '4'}, 728: {'label': '4'}, 2191: {'label': '4'}, 2504: {'label': '4'}, 2290: {'label': '4'}, 1842: {'label': '4'}, 108: {'label': '4'}, 1847: {'label': '1'}, 1850: {'label': '4'}, 444: {'label': '0'}, 1854: {'label': '4'}, 617: {'label': '4'}, 1856: {'label': '2'}, 594: {'label': '1'}, 1857: {'label': '2'}, 131: {'label': '2'}, 663: {'label': '2'}, 141: {'label': '6'}, 1859: {'label': '0'}, 74: {'label': '3'}, 1860: {'label': '1'}, 110: {'label': '1'}, 1862: {'label': '6'}, 111: {'label': '5'}, 2694: {'label': '5'}, 1864: {'label': '6'}, 577: {'label': '6'}, 2164: {'label': '6'}, 2177: {'label': '6'}, 2209: {'label': '6'}, 1865: {'label': '2'}, 448: {'label': '2'}, 1866: {'label': '5'}, 449: {'label': '5'}, 1867: {'label': '5'}, 787: {'label': '5'}, 1871: {'label': '3'}, 2078: {'label': '3'}, 1875: {'label': '0'}, 1877: {'label': '2'}, 1879: {'label': '0'}, 144: {'label': '0'}, 1880: {'label': '2'}, 1884: {'label': '2'}, 1882: {'label': '2'}, 829: {'label': '2'}, 1886: {'label': '1'}, 1887: {'label': '5'}, 535: {'label': '5'}, 1888: {'label': '1'}, 818: {'label': '1'}, 1892: {'label': '1'}, 464: {'label': '3'}, 1893: {'label': '3'}, 1895: {'label': '6'}, 458: {'label': '4'}, 1903: {'label': '6'}, 1905: {'label': '6'}, 1910: {'label': '4'}, 1911: {'label': '4'}, 1913: {'label': '5'}, 2536: {'label': '3'}, 634: {'label': '5'}, 864: {'label': '5'}, 2049: {'label': '5'}, 2304: {'label': '5'}, 1928: {'label': '4'}, 70: {'label': '4'}, 268: {'label': '4'}, 1930: {'label': '4'}, 2618: {'label': '4'}, 1929: {'label': '2'}, 424: {'label': '3'}, 547: {'label': '3'}, 1935: {'label': '5'}, 112: {'label': '5'}, 1936: {'label': '1'}, 139: {'label': '4'}, 1941: {'label': '3'}, 2478: {'label': '3'}, 1943: {'label': '4'}, 706: {'label': '4'}, 1944: {'label': '4'}, 469: {'label': '5'}, 2043: {'label': '5'}, 1951: {'label': '2'}, 1952: {'label': '4'}, 1953: {'label': '0'}, 1918: {'label': '3'}, 471: {'label': '4'}, 1955: {'label': '1'}, 1956: {'label': '4'}, 472: {'label': '4'}, 1957: {'label': '1'}, 1958: {'label': '1'}, 115: {'label': '1'}, 1960: {'label': '1'}, 1959: {'label': '1'}, 262: {'label': '1'}, 474: {'label': '0'}, 1962: {'label': '3'}, 476: {'label': '3'}, 1968: {'label': '4'}, 680: {'label': '4'}, 2384: {'label': '4'}, 1969: {'label': '1'}, 1971: {'label': '4'}, 1975: {'label': '2'}, 839: {'label': '0'}, 1976: {'label': '1'}, 483: {'label': '1'}, 1038: {'label': '1'}, 2511: {'label': '1'}, 2630: {'label': '1'}, 1980: {'label': '2'}, 739: {'label': '5'}, 2096: {'label': '5'}, 537: {'label': '5'}, 2115: {'label': '5'}, 2117: {'label': '5'}, 1983: {'label': '5'}, 2116: {'label': '5'}, 2193: {'label': '5'}, 1984: {'label': '1'}, 514: {'label': '1'}, 587: {'label': '1'}, 1995: {'label': '3'}, 1987: {'label': '1'}, 548: {'label': '1'}, 1988: {'label': '2'}, 117: {'label': '2'}, 1991: {'label': '3'}, 360: {'label': '3'}, 1998: {'label': '5'}, 2004: {'label': '4'}, 2299: {'label': '4'}, 2008: {'label': '4'}, 383: {'label': '3'}, 2011: {'label': '1'}, 488: {'label': '1'}, 2014: {'label': '4'}, 2012: {'label': '4'}, 489: {'label': '4'}, 2013: {'label': '4'}, 490: {'label': '4'}, 491: {'label': '4'}, 2016: {'label': '0'}, 2163: {'label': '0'}, 494: {'label': '1'}, 2023: {'label': '6'}, 840: {'label': '5'}, 2027: {'label': '5'}, 558: {'label': '5'}, 676: {'label': '5'}, 791: {'label': '5'}, 2028: {'label': '2'}, 2413: {'label': '2'}, 2029: {'label': '2'}, 749: {'label': '2'}, 2111: {'label': '5'}, 2151: {'label': '5'}, 2654: {'label': '5'}, 652: {'label': '5'}, 2042: {'label': '5'}, 502: {'label': '5'}, 503: {'label': '5'}, 2046: {'label': '5'}, 505: {'label': '5'}, 2082: {'label': '5'}, 2056: {'label': '1'}, 852: {'label': '5'}, 2060: {'label': '5'}, 529: {'label': '5'}, 536: {'label': '5'}, 2112: {'label': '5'}, 2113: {'label': '5'}, 2066: {'label': '5'}, 2067: {'label': '2'}, 375: {'label': '2'}, 2068: {'label': '1'}, 2410: {'label': '1'}, 516: {'label': '2'}, 2072: {'label': '1'}, 2074: {'label': '0'}, 2076: {'label': '3'}, 2602: {'label': '5'}, 2077: {'label': '5'}, 1197: {'label': '6'}, 236: {'label': '1'}, 524: {'label': '0'}, 2094: {'label': '5'}, 2095: {'label': '5'}, 2107: {'label': '5'}, 2100: {'label': '5'}, 541: {'label': '5'}, 2102: {'label': '5'}, 2103: {'label': '5'}, 2104: {'label': '5'}, 849: {'label': '5'}, 2118: {'label': '5'}, 2120: {'label': '5'}, 2122: {'label': '5'}, 543: {'label': '5'}, 2124: {'label': '5'}, 545: {'label': '5'}, 2125: {'label': '5'}, 2135: {'label': '2'}, 2136: {'label': '5'}, 2687: {'label': '5'}, 2139: {'label': '1'}, 2140: {'label': '5'}, 551: {'label': '5'}, 552: {'label': '5'}, 553: {'label': '5'}, 2146: {'label': '1'}, 556: {'label': '1'}, 2155: {'label': '1'}, 858: {'label': '1'}, 2157: {'label': '5'}, 2160: {'label': '1'}, 2354: {'label': '1'}, 2161: {'label': '5'}, 2162: {'label': '4'}, 569: {'label': '4'}, 165: {'label': '0'}, 2652: {'label': '0'}, 1573: {'label': '1'}, 2168: {'label': '0'}, 2171: {'label': '1'}, 571: {'label': '1'}, 2172: {'label': '1'}, 572: {'label': '1'}, 2175: {'label': '6'}, 450: {'label': '6'}, 2179: {'label': '6'}, 326: {'label': '6'}, 2180: {'label': '1'}, 573: {'label': '4'}, 2181: {'label': '6'}, 644: {'label': '1'}, 2187: {'label': '1'}, 2189: {'label': '2'}, 55: {'label': '2'}, 457: {'label': '4'}, 574: {'label': '2'}, 2190: {'label': '3'}, 182: {'label': '3'}, 188: {'label': '4'}, 2192: {'label': '0'}, 1567: {'label': '0'}, 2194: {'label': '4'}, 575: {'label': '4'}, 859: {'label': '4'}, 2195: {'label': '4'}, 2196: {'label': '4'}, 2197: {'label': '1'}, 1605: {'label': '1'}, 2198: {'label': '1'}, 2201: {'label': '1'}, 2202: {'label': '1'}, 586: {'label': '1'}, 2213: {'label': '1'}, 2216: {'label': '3'}, 422: {'label': '3'}, 2220: {'label': '4'}, 2221: {'label': '4'}, 2222: {'label': '4'}, 2226: {'label': '3'}, 2227: {'label': '3'}, 269: {'label': '1'}, 2228: {'label': '3'}, 147: {'label': '3'}, 2565: {'label': '0'}, 1531: {'label': '6'}, 2240: {'label': '3'}, 127: {'label': '0'}, 2243: {'label': '6'}, 2415: {'label': '0'}, 600: {'label': '1'}, 2357: {'label': '1'}, 2358: {'label': '1'}, 2249: {'label': '1'}, 601: {'label': '1'}, 2250: {'label': '4'}, 602: {'label': '4'}, 616: {'label': '4'}, 2251: {'label': '4'}, 2252: {'label': '4'}, 2277: {'label': '4'}, 2254: {'label': '4'}, 605: {'label': '4'}, 2279: {'label': '4'}, 2257: {'label': '4'}, 606: {'label': '4'}, 623: {'label': '4'}, 629: {'label': '4'}, 2265: {'label': '4'}, 2266: {'label': '4'}, 2285: {'label': '4'}, 2291: {'label': '4'}, 2296: {'label': '4'}, 2297: {'label': '4'}, 2258: {'label': '1'}, 607: {'label': '1'}, 2259: {'label': '1'}, 608: {'label': '4'}, 609: {'label': '4'}, 2262: {'label': '2'}, 615: {'label': '4'}, 624: {'label': '4'}, 628: {'label': '4'}, 2278: {'label': '4'}, 2281: {'label': '4'}, 2268: {'label': '4'}, 2271: {'label': '1'}, 2270: {'label': '1'}, 2274: {'label': '4'}, 2284: {'label': '4'}, 2275: {'label': '4'}, 2276: {'label': '4'}, 626: {'label': '4'}, 2280: {'label': '4'}, 2283: {'label': '4'}, 862: {'label': '4'}, 2287: {'label': '4'}, 2288: {'label': '6'}, 2289: {'label': '4'}, 2406: {'label': '4'}, 2295: {'label': '4'}, 2298: {'label': '4'}, 627: {'label': '4'}, 2303: {'label': '5'}, 632: {'label': '5'}, 2305: {'label': '5'}, 2309: {'label': '1'}, 637: {'label': '1'}, 2310: {'label': '1'}, 636: {'label': '1'}, 2311: {'label': '1'}, 2313: {'label': '1'}, 635: {'label': '1'}, 2318: {'label': '4'}, 2320: {'label': '5'}, 2325: {'label': '0'}, 700: {'label': '0'}, 78: {'label': '1'}, 653: {'label': '5'}, 2329: {'label': '5'}, 2330: {'label': '5'}, 657: {'label': '5'}, 658: {'label': '3'}, 2335: {'label': '1'}, 659: {'label': '1'}, 2342: {'label': '5'}, 2344: {'label': '0'}, 1715: {'label': '0'}, 2345: {'label': '2'}, 744: {'label': '2'}, 132: {'label': '1'}, 811: {'label': '1'}, 2556: {'label': '5'}, 2349: {'label': '6'}, 354: {'label': '1'}, 664: {'label': '1'}, 668: {'label': '1'}, 2361: {'label': '1'}, 133: {'label': '1'}, 2362: {'label': '2'}, 582: {'label': '3'}, 1917: {'label': '0'}, 2690: {'label': '3'}, 673: {'label': '1'}, 2365: {'label': '1'}, 2366: {'label': '4'}, 2371: {'label': '1'}, 2372: {'label': '2'}, 2373: {'label': '1'}, 2374: {'label': '4'}, 2375: {'label': '3'}, 2376: {'label': '4'}, 2378: {'label': '1'}, 2379: {'label': '4'}, 679: {'label': '4'}, 2382: {'label': '4'}, 2387: {'label': '1'}, 2398: {'label': '1'}, 2400: {'label': '1'}, 686: {'label': '1'}, 2401: {'label': '1'}, 119: {'label': '4'}, 613: {'label': '4'}, 2429: {'label': '4'}, 1581: {'label': '0'}, 2427: {'label': '1'}, 2430: {'label': '1'}, 419: {'label': '1'}, 2433: {'label': '3'}, 2434: {'label': '6'}, 2435: {'label': '6'}, 2437: {'label': '0'}, 702: {'label': '0'}, 2438: {'label': '1'}, 703: {'label': '1'}, 2098: {'label': '5'}, 2442: {'label': '1'}, 704: {'label': '1'}, 2443: {'label': '1'}, 2444: {'label': '4'}, 872: {'label': '4'}, 2451: {'label': '4'}, 2452: {'label': '1'}, 713: {'label': '1'}, 372: {'label': '1'}, 2455: {'label': '6'}, 2456: {'label': '1'}, 734: {'label': '1'}, 2518: {'label': '1'}, 2458: {'label': '0'}, 2621: {'label': '0'}, 2671: {'label': '0'}, 2460: {'label': '1'}, 2517: {'label': '1'}, 2464: {'label': '4'}, 2466: {'label': '3'}, 142: {'label': '3'}, 2468: {'label': '1'}, 2469: {'label': '1'}, 2470: {'label': '1'}, 2472: {'label': '1'}, 2473: {'label': '1'}, 2475: {'label': '1'}, 719: {'label': '1'}, 2476: {'label': '1'}, 2479: {'label': '1'}, 760: {'label': '1'}, 2488: {'label': '4'}, 407: {'label': '4'}, 2489: {'label': '4'}, 143: {'label': '4'}, 2490: {'label': '4'}, 2491: {'label': '1'}, 2492: {'label': '3'}, 2495: {'label': '1'}, 725: {'label': '1'}, 2496: {'label': '2'}, 737: {'label': '2'}, 738: {'label': '2'}, 2500: {'label': '2'}, 2519: {'label': '2'}, 2499: {'label': '1'}, 2502: {'label': '1'}, 2543: {'label': '1'}, 280: {'label': '1'}, 631: {'label': '1'}, 2503: {'label': '1'}, 726: {'label': '1'}, 2506: {'label': '1'}, 727: {'label': '1'}, 735: {'label': '1'}, 2509: {'label': '6'}, 2510: {'label': '6'}, 2514: {'label': '1'}, 2515: {'label': '1'}, 736: {'label': '1'}, 2520: {'label': '2'}, 2521: {'label': '2'}, 2527: {'label': '1'}, 741: {'label': '1'}, 2529: {'label': '1'}, 2684: {'label': '5'}, 2533: {'label': '1'}, 2336: {'label': '1'}, 432: {'label': '4'}, 639: {'label': '4'}, 2542: {'label': '1'}, 2544: {'label': '1'}, 2547: {'label': '0'}, 746: {'label': '0'}, 2548: {'label': '1'}, 2552: {'label': '1'}, 2553: {'label': '1'}, 2559: {'label': '2'}, 2692: {'label': '2'}, 2560: {'label': '1'}, 750: {'label': '1'}, 2562: {'label': '3'}, 2563: {'label': '5'}, 2569: {'label': '1'}, 152: {'label': '2'}, 689: {'label': '5'}, 1938: {'label': '2'}, 756: {'label': '1'}, 2576: {'label': '1'}, 2581: {'label': '1'}, 2577: {'label': '1'}, 2579: {'label': '1'}, 2582: {'label': '1'}, 2047: {'label': '5'}, 2589: {'label': '0'}, 1146: {'label': '0'}, 2590: {'label': '0'}, 2591: {'label': '1'}, 2592: {'label': '1'}, 2593: {'label': '2'}, 695: {'label': '6'}, 79: {'label': '2'}, 295: {'label': '1'}, 2598: {'label': '2'}, 2088: {'label': '3'}, 2617: {'label': '3'}, 838: {'label': '4'}, 1770: {'label': '5'}, 2153: {'label': '5'}, 2535: {'label': '5'}, 2612: {'label': '6'}, 2613: {'label': '5'}, 332: {'label': '1'}, 779: {'label': '2'}, 1389: {'label': '2'}, 2620: {'label': '6'}, 692: {'label': '2'}, 2624: {'label': '1'}, 2662: {'label': '1'}, 2629: {'label': '4'}, 2638: {'label': '4'}, 2640: {'label': '2'}, 2641: {'label': '1'}, 2644: {'label': '3'}, 973: {'label': '3'}, 2645: {'label': '2'}, 2647: {'label': '3'}, 2648: {'label': '3'}, 253: {'label': '1'}, 2653: {'label': '5'}, 2669: {'label': '3'}, 2675: {'label': '6'}, 2677: {'label': '0'}, 81: {'label': '0'}, 1428: {'label': '0'}, 2678: {'label': '6'}, 2679: {'label': '6'}, 2680: {'label': '3'}, 2704: {'label': '3'}, 2119: {'label': '5'}, 2689: {'label': '5'}, 2691: {'label': '2'}, 1547: {'label': '2'}})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real = np.asarray(classes)\n",
        "real = real[:,1]\n",
        "real = list(map(str,real))\n",
        "\n",
        "predicted = resulted(G)"
      ],
      "metadata": {
        "id": "tQZZ4RXvHq9b"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "for i in range(len(real)):\n",
        "        if real[i] == predicted[i]:\n",
        "                correct += 1\n",
        "miss = len(real)-correct\n",
        "accuracy = correct/len(real)\n",
        "print(\"Accuracy is %f \" %accuracy)\n",
        "print(\"Hits are %d and misses are %d\" %(correct,miss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSepPugxnpCK",
        "outputId": "0e6b696f-433b-4c8b-e8ec-f0c150f6b1f5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 1.000000 \n",
            "Hits are 2708 and misses are 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsQfEiaI8O0j",
        "outputId": "86379d43-c8a1-4cd1-ee7c-fe1f72ce500b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '1', '3', ..., '1', '1', '1'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}